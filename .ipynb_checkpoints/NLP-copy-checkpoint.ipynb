{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Import</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#import des librairies\n",
    "%pylab inline\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "import re\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from spellchecker import SpellChecker\n",
    "from textblob import TextBlob\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.ensemble import BaggingClassifier as BC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import plot_importance, to_graphviz\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, auc, precision_score, recall_score\n",
    "from sklearn import feature_extraction, model_selection, svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim.utils import tokenize\n",
    "from gensim.models import word2vec\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore les warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chargement des données\n",
    "#remplacement de None par NaN\n",
    "#décimal avec un point plutôt qu'une virgule dans le dataframe afin que les variables soient de type float\n",
    "df = pd.read_csv(r\"C:\\Users\\utilisateur\\Documents\\Projet\\ProjetNLP\\booking.csv\", na_values=['None'], decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>exceptionnel établissement recent propre soign...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>fabuleux accueil très chaleureux chambre calme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>exceptionnel tout bié sauf wifi catastrophique...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>exceptionnel nan hôtel parfait quartier sympa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>fabuleux excellent rapport qualité prix person...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sentiment                                           sentence\n",
       "0           0          1  exceptionnel établissement recent propre soign...\n",
       "1           1          1  fabuleux accueil très chaleureux chambre calme...\n",
       "2           2          1  exceptionnel tout bié sauf wifi catastrophique...\n",
       "3           3          1      exceptionnel nan hôtel parfait quartier sympa\n",
       "4           4          1  fabuleux excellent rapport qualité prix person..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#affichage des 5 premières lignes du dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Exploration et nettoyage des données</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le dataset a 30946 lignes et 3 colonnes.\n"
     ]
    }
   ],
   "source": [
    "#dimensions du dataframe\n",
    "print('Le dataset a {} lignes et {} colonnes.'.format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les différents variables sont : ['Unnamed: 0', 'sentiment', 'sentence'].\n"
     ]
    }
   ],
   "source": [
    "#affichage des variables\n",
    "print('Les différents variables sont : {}.'.format(df.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Existe-t-il des variables non renseignées?\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "sentiment     0\n",
       "sentence      6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#affichage du nombre de valeurs manquantes selon chaque variable\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     int64\n",
       "sentiment      int64\n",
       "sentence      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type de chaque variable\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30946.000000</td>\n",
       "      <td>30946.000000</td>\n",
       "      <td>30940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exceptionnel client  laissé commentaire nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15472.500000</td>\n",
       "      <td>0.830899</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8933.485052</td>\n",
       "      <td>0.374847</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7736.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>15472.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23208.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>30945.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0     sentiment  \\\n",
       "count   30946.000000  30946.000000   \n",
       "unique           NaN           NaN   \n",
       "top              NaN           NaN   \n",
       "freq             NaN           NaN   \n",
       "mean    15472.500000      0.830899   \n",
       "std      8933.485052      0.374847   \n",
       "min         0.000000      0.000000   \n",
       "25%      7736.250000      1.000000   \n",
       "50%     15472.500000      1.000000   \n",
       "75%     23208.750000      1.000000   \n",
       "max     30945.000000      1.000000   \n",
       "\n",
       "                                           sentence  \n",
       "count                                         30940  \n",
       "unique                                        26290  \n",
       "top     exceptionnel client  laissé commentaire nan  \n",
       "freq                                            417  \n",
       "mean                                            NaN  \n",
       "std                                             NaN  \n",
       "min                                             NaN  \n",
       "25%                                             NaN  \n",
       "50%                                             NaN  \n",
       "75%                                             NaN  \n",
       "max                                             NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#statistiques descriptives du dataset\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppression des accents\n",
    "#cols = df.select_dtypes(include=[np.object]).columns\n",
    "#df[cols] = df[cols].apply(lambda x: x.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8'))\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction pour générer un nuage de mots\n",
    "def wcloud(data,bgcolor,title):\n",
    "    plt.figure(figsize = (100,100))\n",
    "    wc = WordCloud(background_color = bgcolor, max_words = 1000,  max_font_size = 50)\n",
    "    wc.generate(' '.join(data))\n",
    "    plt.imshow(wc)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\utilisateur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['nom' 'pays' 'favorite' 'date' 'note' 'type_etablissement' 'lieu'\\n 'note_etablissement'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-bb5486a32f2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nom'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pays'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'favorite'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'note'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'type_etablissement'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lieu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'note_etablissement'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3995\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3996\u001b[0m             \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3997\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3998\u001b[0m         )\n\u001b[0;32m   3999\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3934\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3935\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3936\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3938\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3968\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3969\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3970\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3971\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5016\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5017\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5018\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5019\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5020\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['nom' 'pays' 'favorite' 'date' 'note' 'type_etablissement' 'lieu'\\n 'note_etablissement'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df.drop(['nom', 'pays', 'favorite', 'date', 'note', 'type_etablissement', 'lieu', 'note_etablissement'],1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppression des Nan\n",
    "df = df.fillna('')\n",
    "#suppression de deux phrases récurrentes dans les données\n",
    "df = df[df.bons_points != \"Ce commentaire n'apparaît pas car il ne respecte pas notre charte.\"]\n",
    "df = df[df.bons_points != \"Ce client n'a pas laissé de commentaire.\"]\n",
    "\n",
    "df = df[df.mauvais_points != \"Ce commentaire n'apparaît pas car il ne respecte pas notre charte.\"]\n",
    "df = df[df.mauvais_points != \"Ce client n'a pas laissé de commentaire.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#itération sur l'ensemble des lignes du dataframe, affichache de l'index et son commentaire associé\n",
    "for index, row in df.iterrows():\n",
    "    print('index: ', index, 'col sentences:', row['titre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#itération sur l'ensemble des lignes du dataframe, affichache de l'index et son commentaire associé\n",
    "#for index, row in df.iterrows():\n",
    "    #print('index: ', index, 'col sentences:', row['bons_points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#itération sur l'ensemble des lignes du dataframe, affichache de l'index et son commentaire associé\n",
    "#for index, row in df.iterrows():\n",
    "    #print('index: ', index, 'col sentences:', row['bons_points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str = \"ÊÎÔÛÄËÏÖÜÀÆæÇÉÈŒœÙ!!!\";\n",
    "#print(str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def cleaned_data(x):\n",
    "    #str.lower : mettre tous les éléments en minuscule\n",
    "    #str.cat : concaténation des strings avec le séparateur donné en paramètre\n",
    "    #a = x.str.lower().str.cat(sep=' ')\n",
    "    #print(x)\n",
    "#df.apply(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns = list(df.columns)\n",
    "#print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colum = list(df[column])\n",
    "#for col in columns : \n",
    "    #def[col]=cleaned_data(df[col])\n",
    "    #str.lower : mettre tous les éléments en minuscule\n",
    "    #str.cat : concaténation des strings avec le séparateur donné en paramètre\n",
    "    #a = df[col].str.lower().str.cat(sep=' ')\n",
    "\n",
    "    #supprimer ponctuation, nombres et retourner une liste de mots\n",
    "    #b = re.sub('[^a-zàâéèêëïîôùûçæœ]+', ' ', a)\n",
    "\n",
    "    #supprimer tous les mots \"vides\" du texte\n",
    "    #stop_words = list(get_stop_words('french'))         \n",
    "    #nltk_words = list(stopwords.words('french'))   \n",
    "    #stop_words.extend(nltk_words)\n",
    "\n",
    "    #word_tokens = word_tokenize(b)\n",
    "    #filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    #filtered_sentence = []\n",
    "    #for w in word_tokens:\n",
    "        #if w not in stop_words:\n",
    "            #filtered_sentence.append(w)\n",
    "\n",
    "    #supprimer les mots qui ont une longueur inférieure à 2  \n",
    "    #without_single_chr = [word for word in filtered_sentence if len(word) > 2]\n",
    "\n",
    "    #suppression des caractères numériques\n",
    "    #cleaned_data = [word for word in without_single_chr if not word.isnumeric()]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str.lower : mettre tous les éléments en minuscule\n",
    "#str.cat : concaténation des strings avec le séparateur donné en paramètre\n",
    "a = df['titre'].str.lower().str.cat(sep=' ')\n",
    "\n",
    "#supprimer ponctuation, nombres et retourner une liste de mots\n",
    "b = re.sub('[^a-zàâéèêëïîôùûçæœ]+', ' ', a)\n",
    "\n",
    "#supprimer tous les mots \"vides\" du texte\n",
    "stop_words = list(get_stop_words('french'))         \n",
    "nltk_words = list(stopwords.words('french'))   \n",
    "stop_words.extend(nltk_words)\n",
    "\n",
    "#token = entité (ou unité) lexicale \n",
    "word_tokens = word_tokenize(b)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_sentence = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "#supprimer les mots qui ont une longueur inférieure à 2  \n",
    "without_single_chr = [word for word in filtered_sentence if len(word) > 2]\n",
    "\n",
    "#suppression des caractères numériques\n",
    "cleaned_data = [word for word in without_single_chr if not word.isnumeric()]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculer les 100 mots les plus fréquents\n",
    "top_N = 100\n",
    "word_dist = nltk.FreqDist(cleaned_data)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N), columns=['Word', 'Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affichage dans un dataframe des 7 mots les plus fréquents\n",
    "rslt.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogramme des 7 mots les plus fréquents\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.barplot(x=\"Word\",y=\"Frequency\", data=rslt.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nuage de mots\n",
    "wcloud(cleaned_data,'black','Common Words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mettre tous les éléments en miniscule, séparés par un espace\n",
    "a = df['bons_points'].str.lower().str.cat(sep=' ')\n",
    "\n",
    "#supprimer ponctuation, nombres and retourner une liste de mots\n",
    "b = re.sub('[^a-zàâéèêëïîôùûç]+', ' ', a)\n",
    "\n",
    "#supprimer tous les mots \"vides\" du texte\n",
    "stop_words = list(get_stop_words('french'))         \n",
    "nltk_words = list(stopwords.words('french'))   \n",
    "stop_words.extend(nltk_words)\n",
    "\n",
    "word_tokens = word_tokenize(b)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_sentence = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "#supprimer les mots qui ont une longueur inférieure à 2  \n",
    "without_single_chr = [word for word in filtered_sentence if len(word) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supprimer les caractères numériques\n",
    "cleaned_data = [word for word in without_single_chr if not word.isnumeric()]        \n",
    "\n",
    "#calculer les 100 mots les plus fréquents\n",
    "top_N = 100\n",
    "word_dist = nltk.FreqDist(cleaned_data)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N), columns=['Word', 'Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affichage dans un dataframe des 7 mots les plus fréquents\n",
    "rslt.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogramme des 7 mots les plus fréquents\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.barplot(x=\"Word\",y=\"Frequency\", data=rslt.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nuage de mots\n",
    "wcloud(cleaned_data,'black','Common Words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mettre tous les éléments en miniscule, séparés par un espace\n",
    "a = df['mauvais_points'].str.lower().str.cat(sep=' ')\n",
    "\n",
    "#supprimer ponctuation, nombres and retourner une liste de mots\n",
    "b = re.sub('[^A-Za-zàâéèêëïîôùûç]+', ' ', a)\n",
    "\n",
    "#supprimer tous les mots \"vides\" du texte\n",
    "stop_words = list(get_stop_words('french'))         \n",
    "nltk_words = list(stopwords.words('french'))   \n",
    "stop_words.extend(nltk_words)\n",
    "\n",
    "word_tokens = word_tokenize(b)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_sentence = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "#supprimer les mots qui ont une longueur inférieure à 2  \n",
    "without_single_chr = [word for word in filtered_sentence if len(word) > 2]\n",
    "\n",
    "#supprimer caractères numériques\n",
    "cleaned_data = [word for word in without_single_chr if not word.isnumeric()]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supprimer caractères numériques\n",
    "cleaned_data = [word for word in without_single_chr if not word.isnumeric()]        \n",
    "\n",
    "#calculer les 100 mots les plus fréquents\n",
    "top_N = 100\n",
    "word_dist = nltk.FreqDist(cleaned_data)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N), columns=['Word', 'Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affichage dans un dataframe des 7 mots les plus fréquents\n",
    "rslt.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogramme des 7 mots les plus fréquents\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.barplot(x=\"Word\",y=\"Frequency\", data=rslt.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nuage de mots\n",
    "wcloud(cleaned_data,'black','Common Words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vérification que les NaN dans la colonne 'mauvais points' ont bien de façon certaine une polarité de zéro pour TextBlob, \n",
    "#sentiment[0]=polarite et sentiment[1]=subjectivité\n",
    "#text = u\"NaN\"\n",
    "#blob = TextBlob(text, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "#blob.sentiment\n",
    "#print('NaN a une polarité de {} et une subjectivité de {}.'.format(blob.sentiment[0], blob.sentiment[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppression des Nan\n",
    "#df = df.fillna('')\n",
    "#suppression de deux phrases récurrentes dans les données\n",
    "#df = df[df.bons_points != \"Ce commentaire n'apparaît pas car il ne respecte pas notre charte.\"]\n",
    "#df = df[df.bons_points != \"Ce client n'a pas laissé de commentaire.\"]\n",
    "\n",
    "#df = df[df.mauvais_points != \"Ce commentaire n'apparaît pas car il ne respecte pas notre charte.\"]\n",
    "#df = df[df.mauvais_points != \"Ce client n'a pas laissé de commentaire.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppression de deux phrases récurrentes dans les données\n",
    "#df = df[df.bons_points != \"Ce commentaire n'apparaît pas car il ne respecte pas notre charte.\"]\n",
    "#df = df[df.bons_points != \"Ce client n'a pas laissé de commentaire.\"]\n",
    "\n",
    "#df = df[df.mauvais_points != \"Ce commentaire n'apparaît pas car il ne respecte pas notre charte.\"]\n",
    "#df = df[df.mauvais_points != \"Ce client n'a pas laissé de commentaire.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concaténation \n",
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "df22 = pd.DataFrame(df, columns= ['bons_points', 'mauvais_points'])\n",
    "\n",
    "df23=df22['bons_points']\n",
    "df24=df22['mauvais_points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df23=pd.DataFrame(data=df23)\n",
    "#df23['polarite']=1\n",
    "df23.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df24=pd.DataFrame(data=df24)\n",
    "#df24['polarité']=0\n",
    "df24.columns = ['bons_points']\n",
    "df24.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df23, df24]\n",
    "result = pd.concat(frames)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df21 = result['bons_points']\n",
    "df21 = pd.DataFrame(df21, columns=['bons_points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df21.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df21.columns=['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#mettre les commentaires du dataframe en miniscule\n",
    "df21['sentences']=df21['sentences'].str.lower()\n",
    "df21.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df21.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remplacement de tout ce qui n'est pas dans le set[] par un espace\n",
    "df21['sentences'] = df21['sentences'].apply(lambda x: re.sub('[^a-zàâéèêëïîôùûçæœ-]+',' ', str(x))) \n",
    "#for index, row in df21.iterrows():\n",
    "    #print('index: ', index, 'col sentences:', row['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppression des mots \"vides\" dans le dataframe\n",
    "df21['sentences'] = df21['sentences'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#for index, row in df21.iterrows():\n",
    "    #print('index: ', index, 'col sentences:', row['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppression des mots d'une longueur inférieure à 2 caractères\n",
    "#\\b=la fin et le début d'un mot\n",
    "#`r' ou`R': préfixe pour les chaines de caractère \n",
    "df21['sentences']=df21.sentences.str.replace(r'\\b(\\w{1,2})\\b', '')\n",
    "#for index, row in df21.iterrows():\n",
    "    #print('index: ', index, 'col sentences:', row['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='french')\n",
    "\n",
    "def return_stem(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [stemmer.stem(X.text) for X in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df22=df21.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df22.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Existe-t-il des variables numériques?\n",
    "df21['sentences'].apply(lambda x: not any(i.isnumeric() for i in x.split())).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df21['sentences'].str.contains(r'[0-9]').any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#récupération des commentaires dans le type series\n",
    "df21 = df21['sentences']\n",
    "type(df21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcul de la polarité des commentaires avec TextBlob\n",
    "bloblist_desc = list()\n",
    "\n",
    "df_str=df21.astype(str)\n",
    "for row in df_str:\n",
    "    blob = TextBlob(row)\n",
    "    #print(blob)\n",
    "    pos_tagger=PatternTagger()\n",
    "    analyzer=PatternAnalyzer()\n",
    "    blob = tb(str(blob)) #textblob\n",
    "    #sentiment renvoie un tuple avec la polarité et la subjectivite\n",
    "    #print(blob.sentiment[0])\n",
    "    #type(blob.sentiment[0])\n",
    "    bloblist_desc.append(blob.sentiment[0])\n",
    "    #df_polarity_desc1 = pd.DataFrame(bloblist_desc, columns = ['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#récupération des résultats de la polarité dans un dataframe\n",
    "df_polarity_desc1 = pd.DataFrame(data = bloblist_desc)\n",
    "df_polarity_desc1.columns = ['sentiment']\n",
    "df_polarity_desc1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification des commentaires selon trois modalités (neutres, positifs et négatifs) dans un dataframe\n",
    "def f(df_polarity_desc1):\n",
    "    if df_polarity_desc1['sentiment'] > 0:\n",
    "        val = \"positive\"\n",
    "    elif df_polarity_desc1['sentiment'] < 0:\n",
    "        val = \"negative\"\n",
    "    else :\n",
    "        val = \"neutre\"\n",
    "    return val\n",
    "\n",
    "df_polarity_desc1.apply(f, axis=1)\n",
    "df_polarity_desc1['polarite']=df_polarity_desc1.apply(f, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polarity_desc1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction de masse de la polarité des commentaires\n",
    "num_bins = 50\n",
    "plt.figure(figsize=(10,6))\n",
    "n, bins, patches = plt.hist(df_polarity_desc1.sentiment, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of polarity')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxenplot de la polarité des commenatires, boxplot avec plus de quartiles\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxenplot(x='polarite', y='sentiment', data=df_polarity_desc1)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countplot des modalités correspondant à la polarité des commentaires\n",
    "df_polarity_desc1.head()\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.countplot(x=\"polarite\", data=df_polarity_desc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diagramme circulaire des modalités de la polarité des commentaires\n",
    "pourcentage_positive = round(len(df_polarity_desc1.loc[(df_polarity_desc1.polarite == \"positive\")])) / len(df_polarity_desc1)\n",
    "pourcentage_negative = round(len(df_polarity_desc1.loc[(df_polarity_desc1.polarite == \"negative\")])) / len(df_polarity_desc1)\n",
    "pourcentage_neutre = round(len(df_polarity_desc1.loc[(df_polarity_desc1.polarite == \"neutre\")])) / len(df_polarity_desc1)\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "\n",
    "plt.title(\"Pourcentage des commentaires positifs vs négatifs vs neutres\", fontsize=18)\n",
    "labels = ['positif', 'négatif', 'neutre']\n",
    "sizes = [pourcentage_positive, pourcentage_negative, pourcentage_neutre]\n",
    "#explode = (0, 0.2)  # seulement détacher le second groupe\n",
    "\n",
    "patches, texts, autotexts = ax1.pie(sizes,  labels=labels, autopct='%1.1f%%', shadow = True, startangle=130, colors = ['#00e64d', 'r', '#FFD700'])\n",
    "texts[0].set_fontsize(15)\n",
    "texts[1].set_fontsize(15)\n",
    "texts[2].set_fontsize(15)\n",
    "\n",
    "matplotlib.rcParams['text.color'] = 'black'\n",
    "matplotlib.rcParams[\"font.size\"] = 18\n",
    "plt.rcParams[\"figure.figsize\"] = [6, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\utilisateur\\Documents\\Projet\\ProjetNLP\\booking.csv\", na_values=['None'], decimal=',')\n",
    "df23=df['note']\n",
    "df24=df['note']\n",
    "frames = [df23, df24]\n",
    "df_notes = pd.concat(frames)\n",
    "df_notes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polarity_desc1_polarite = df_polarity_desc1.iloc[:,1]\n",
    "#df_polarity_desc1_polarite = pd.DataFrame(df_polarity_desc1_polarite)\n",
    "df_polarity_desc1_polarite.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes.reset_index(drop=True, inplace=True)\n",
    "df_polarity_desc1_polarite.reset_index(drop=True, inplace=True)\n",
    "df_corr = pd.concat([df_polarity_desc1_polarite, df_notes], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr=df_corr.loc[(df_corr.polarite=='positive')|(df_corr.polarite=='negative')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr['polarite'] = np.where(df_corr['polarite'] == 'positive', 1, 0)\n",
    "df_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affichage des corrélations entre variables\n",
    "#parfaite si r = 1\n",
    "#très forte si r > 0,8\n",
    "#forte si r se situe entre 0,5 et 0,8\n",
    "#d'intensité moyenne si r se situe entre 0,2 et 0,5\n",
    "#faible si r se situe entre 0 et 0.2\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "plt.title('Correlation entre variables')\n",
    "sns.heatmap(df_corr.corr(),linewidths=0.25,vmax=1.0, square=True, cmap=\"YlGnBu\", linecolor='black', annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concaténation des trois colonnes correspondant aux commentaires sur les hôtels\n",
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "df22 = pd.DataFrame(df, columns= ['titre', 'bons_points', 'mauvais_points'])\n",
    "\n",
    "df21 = df22['titre'].map(str) + ' ' + df22['bons_points'].map(str) + ' ' + df22['mauvais_points'].map(str)\n",
    "\n",
    "df21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertir series en dataframe\n",
    "df21 = df21.to_frame(name=\"sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mettre les commentaires du dataframe en miniscule\n",
    "df21['sentences']=df21['sentences'].str.lower()\n",
    "df21.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remplacement de tout ce qui n'est pas dans le set[] par un espace\n",
    "df21['sentences'] = df21['sentences'].apply(lambda x: re.sub('[^a-zàâéèêëïîôùûçæœ-]+',' ', str(x))) \n",
    "#for index, row in df21.iterrows():\n",
    "    #print('index: ', index, 'col sentences:', row['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppression des mots \"vides\" dans le dataframe\n",
    "stop = stopwords.words('french')\n",
    "df21['sentences'] = df21['sentences'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#for index, row in df21.iterrows():\n",
    "    #print('index: ', index, 'col sentences:', row['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppression des mots d'une longueur inférieure à 2 caractères\n",
    "#\\b=la fin et le début d'un mot\n",
    "#`r' ou`R': préfixe pour les chaines de caractère \n",
    "df21['sentences']=df21.sentences.str.replace(r'\\b(\\w{1,2})\\b', '')\n",
    "#for index, row in df21.iterrows():\n",
    "    #print('index: ', index, 'col sentences:', row['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df22=df21.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df22.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Existe-t-il des variables numériques?\n",
    "df21['sentences'].apply(lambda x: not any(i.isnumeric() for i in x.split())).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#récupération des commentaires dans le type series\n",
    "df21 = df21['sentences']\n",
    "type(df21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcul de la polarité des commentaires avec TextBlob\n",
    "bloblist_desc = list()\n",
    "\n",
    "df_str=df21.astype(str)\n",
    "for row in df_str:\n",
    "    blob = TextBlob(row)\n",
    "    #print(blob)\n",
    "    pos_tagger=PatternTagger()\n",
    "    analyzer=PatternAnalyzer()\n",
    "    blob = tb(str(blob)) #textblob\n",
    "    #sentiment renvoie un tuple avec la polarité et la subjectivite\n",
    "    #print(blob.sentiment[0])\n",
    "    #type(blob.sentiment[0])\n",
    "    bloblist_desc.append(blob.sentiment[0])\n",
    "    #df_polarity_desc1 = pd.DataFrame(bloblist_desc, columns = ['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#récupération des résultats de la polarité dans un dataframe\n",
    "df_polarity_desc1 = pd.DataFrame(data = bloblist_desc)\n",
    "df_polarity_desc1.columns = ['sentiment']\n",
    "df_polarity_desc1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification des commentaires selon trois modalités (neutres, positifs et négatifs) dans un dataframe\n",
    "def f(df_polarity_desc1):\n",
    "    if df_polarity_desc1['sentiment'] > 0:\n",
    "        val = \"positive\"\n",
    "    elif df_polarity_desc1['sentiment'] < 0:\n",
    "        val = \"negative\"\n",
    "    else :\n",
    "        val = \"neutre\"\n",
    "    return val\n",
    "\n",
    "df_polarity_desc1.apply(f, axis=1)\n",
    "df_polarity_desc1['polarite']=df_polarity_desc1.apply(f, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sélection de la colonne notes \n",
    "df = pd.read_csv(r\"C:\\Users\\utilisateur\\Documents\\Projet\\ProjetNLP\\booking.csv\", na_values=['None'], decimal=',')\n",
    "df_notes = df.iloc[:,7]\n",
    "#df_notes = pd.DataFrame(data=df_notes)\n",
    "df_notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sélection de la colonne polarite\n",
    "df_polarity_desc1_polarite = df_polarity_desc1.iloc[:,1]\n",
    "#df_polarity_desc1_polarite = pd.DataFrame(df_polarity_desc1_polarite)\n",
    "df_polarity_desc1_polarite.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concaténation de la colonne notes et polarite\n",
    "df_corr = pd.concat([df_polarity_desc1_polarite, df_notes], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sélection des modalités postives ou négatives\n",
    "df_corr=df_corr.loc[(df_corr.polarite=='positive')|(df_corr.polarite=='negative')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformation des valeurs de la polarite en code binaire 0 et 1\n",
    "df_corr['polarite'] = np.where(df_corr['polarite'] == 'positive', 1, 0)\n",
    "df_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affichage des corrélations entre variables\n",
    "#parfaite si r = 1\n",
    "#très forte si r > 0,8\n",
    "#forte si r se situe entre 0,5 et 0,8\n",
    "#d'intensité moyenne si r se situe entre 0,2 et 0,5\n",
    "#faible si r se situe entre 0 et 0.2\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "plt.title('Correlation entre variables')\n",
    "sns.heatmap(df_corr.corr(),linewidths=0.25,vmax=1.0, square=True, cmap=\"YlGnBu\", linecolor='black', annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_polarity_desc1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_polarity_desc1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformation des valeurs de la variable polarite en 1 et 0\n",
    "#pas besoin d'utiliser le module scikit-learn car on peut le faire directement \n",
    "df_polarity_desc1['polarite'] = np.where(df_polarity_desc1['polarite'] == 'positive', 1, 0)\n",
    "df_polarity_desc1 = df_polarity_desc1.reset_index(drop=True)\n",
    "df_polarity_desc1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supression de la colonne sentiment\n",
    "df_polarity_desc1.drop(['sentiment'],1,inplace=True)\n",
    "df_polarity_desc1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#création d'un dataframe comments avec la colonne sentences\n",
    "df21.head()\n",
    "comments = pd.DataFrame(data=df21)\n",
    "comments.columns = ['sentences']\n",
    "comments.head()\n",
    "comments.shape\n",
    "type(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_polarity_desc1.shape\n",
    "#type(df_polarity_desc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concaténation de la colonne sentences et la colonne polarite\n",
    "dataset = pd.concat([comments, df_polarity_desc1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.reset_index(drop=True, inplace=True)\n",
    "df_polarity_desc1.reset_index(drop=True, inplace=True)\n",
    "# fusion des deux datasets en un seul que l'on nommera df\n",
    "df = pd.concat([df_polarity_desc1, comments], axis = 1)\n",
    "#df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renommer les colonnes\n",
    "df.columns=['sentiment', 'sentence']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supression des données manquantes\n",
    "df.isnull().values.any()\n",
    "df.isnull().sum()\n",
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vérification des données manquantes\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#création d'un csv contenant le dataframe df utilisé pour le machine learning\n",
    "df.to_csv('booking.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Machine learning</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#création jeux train et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['sentence']], df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer de Scikit-learn est utilisé pour transformer un corpus de mots en vecteurs/occurence des mots \n",
    "#Tf signifie term-frequency tandis que tfidf signifie inverse document-frequency\n",
    "#IDF(word) = Log((Total number of documents)/(Number of documents containing the word))\n",
    "pipe = make_pipeline(CountVectorizer(), TfidfTransformer())\n",
    "pipe.fit(X_train['sentence'])\n",
    "feat_train = pipe.transform(X_train['sentence'])\n",
    "feat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimum et maximum pour feat_train\n",
    "feat_train.min(), feat_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer sans entrainer sur notre jeu test\n",
    "feat_test = pipe.transform(X_test['sentence'])\n",
    "feat_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arbre de décision\n",
    "#définir les paramètres\n",
    "#le paramètre max_depth est un seuil sur la profondeur maximale de l’arbre\n",
    "\"\"\"L'indice de diversité de gini : probabilité pour chaque élément d'être choisi multipliée \n",
    "par la probabilité qu'il soit mal classé\"\"\"\n",
    "\"\"\"L'entropie au lieu d'utiliser les probabilités simples applique le log2 des probabilités\"\"\"\n",
    "param_grid = {'max_depth' : [400, 500], \n",
    "              'criterion' : ['gini', 'entropy']}\n",
    "             \n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "dtc_grid = GridSearchCV(dtc, param_grid, cv=3)\n",
    " \n",
    "#entrainer le modèle à partir de Grid Search\n",
    "%time DTC = dtc_grid.fit(feat_train, y_train)\n",
    "\n",
    "print(dtc_grid.best_score_.round(5))\n",
    "print(dtc_grid.best_params_)\n",
    "\n",
    "final_model = dtc_grid.best_estimator_\n",
    "\n",
    "pred_train = final_model.predict(feat_train) \n",
    "pred_test = final_model.predict(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatriceConfusion(model):\n",
    "    y_pred = model.predict(feat_test)\n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, pred_test)\n",
    "    plt.matshow(cm)\n",
    "    plt.title('Matrice de confusion', y=1.12)\n",
    "    plt.colorbar()\n",
    "    print(cm)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = MatriceConfusion(DTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = final_model.predict_proba(feat_test)\n",
    "fpr, tpr, th = roc_curve(y_test, score[:, 1])\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "aucf = auc(fpr, tpr)\n",
    "ax.plot(fpr, tpr, label='auc=%1.5f' % aucf)\n",
    "ax.set_title('Courbe ROC - classifieur de sentiments')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"n_estimators = le nombre d'arbres dans la fôrét\"\"\"\n",
    "\"\"\"Le Bootstrapping est un procédé qui permet d’augmenter artificiellement le nombre d’observation d’un \n",
    "échantillon de données sans pour autant modifier la distribution des variables présentes dans le jeu de \n",
    "données. Le principe est simple, on dispose d’un jeu de données contenant n observations, pour créer un \n",
    "échantillon de taille n  on tire avec remise n observations parmi le jeu de données original\"\"\"\n",
    "param_grid = {'max_depth' : [400, 500],\n",
    "             'n_estimators' : [50, 70],\n",
    "              'bootstrap' : [True, False],\n",
    "              'criterion' : ['gini','entropy']}\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rfc_grid = GridSearchCV(rfc, param_grid, cv = 3)\n",
    "\n",
    "%time RFC = rfc_grid.fit(feat_train, y_train)\n",
    "\n",
    "print(rfc_grid.best_score_.round(5))\n",
    "print(rfc_grid.best_params_)\n",
    "\n",
    "final_model = rfc_grid.best_estimator_\n",
    "\n",
    "pred_train = final_model.predict(feat_train) \n",
    "pred_test = final_model.predict(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = MatriceConfusion(RFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = final_model.predict_proba(feat_test)\n",
    "fpr, tpr, th = roc_curve(y_test, score[:, 1])\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "aucf = auc(fpr, tpr)\n",
    "ax.plot(fpr, tpr, label='auc=%1.5f' % aucf)\n",
    "ax.set_title('Courbe ROC - classifieur de sentiments')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"L1 : Lasso et L2 : Ridge, pénalité, c’est une modification qu’on apporte à la fonction de coût \n",
    "afin de maîtriser l’arbitrage entre biais vs variance\"\"\"\n",
    "\"\"\"la fonction de coût est définie comme le carré de la différence entre la valeur prévue et la \n",
    "valeur réelle en fonction de l'intrant.\"\"\"\n",
    "\"\"\"quand le biais augmente, la variance baisse.\"\"\"\n",
    "param_grid = {'penalty' : ['l1', 'l2']}\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "lr_grid = GridSearchCV(lr, param_grid, cv = 3)\n",
    "\n",
    "%time LR = lr_grid.fit(feat_train, y_train)\n",
    "\n",
    "print(lr_grid.best_score_.round(5))\n",
    "print(lr_grid.best_params_)\n",
    "\n",
    "final_model = lr_grid.best_estimator_\n",
    "\n",
    "pred_train = final_model.predict(feat_train) \n",
    "pred_test = final_model.predict(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = MatriceConfusion(LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = final_model.decision_function(feat_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bagging\n",
    "\"\"\"Le mot Bagging est une contraction de Bootstrap Aggregation : sélection de sous-ensembles aléatoires de\n",
    "l'ensemble des données d'origine, puis agrégation des prédictions individuelles pour former une prédiction\n",
    "finale\"\"\" \n",
    "\n",
    "naivebayes = nb.BernoulliNB()\n",
    "modelB = BC(base_estimator = naivebayes, n_estimators = 1000, warm_start = True, bootstrap_features=True, max_samples=0.9)\n",
    "modelB_fit = modelB.fit(feat_train, y_train)\n",
    "modelB_fit.score(feat_test,y_test)\n",
    "pred_train = modelB.predict(feat_train)\n",
    "pred_test = modelB.predict(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = MatriceConfusion(modelB_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = modelB_fit.predict_proba(feat_test)\n",
    "fpr, tpr, th = roc_curve(y_test, score[:, 1])\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "aucf = auc(fpr, tpr)\n",
    "ax.plot(fpr, tpr, label='auc=%1.5f' % aucf)\n",
    "ax.set_title('Courbe ROC - classifieur de sentiments')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {#'C': [10, 50, 100,200],\n",
    "              #'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf','sigmoid']}\n",
    "\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "svm_grid = GridSearchCV(svm, param_grid, cv=3)\n",
    "\n",
    "%time SVM = svm_grid.fit(feat_train, y_train)\n",
    "\n",
    "print(svm_grid.best_score_.round(5))\n",
    "print(svm_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = svm_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = final_model.predict(feat_train) \n",
    "pred_test = final_model.predict(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = MatriceConfusion(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = SVM.decision_function(feat_test)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient boosting = boosting + descente de gradient\n",
    "#fonctionne de manière très similaire au forêts aléatoires pour la classification.\n",
    "#néanmoins, les arbres ne sont plus appris indépendamment des autres \n",
    "\"\"\"Le BOOSTING est une technique ensembliste qui consiste à agréger des classifieurs (modèles) élaborés \n",
    "séquentiellement sur un échantillon d’apprentissage dont les poids des individus sont corrigés au fur et \n",
    "à mesure. Les classifieurs sont pondérés selon leurs performances\"\"\"\n",
    "gbc40 = GradientBoostingClassifier(n_estimators=50, max_depth=500, learning_rate=0.08)\n",
    "gbc40.fit(feat_train, y_train)\n",
    "gbc40.score(feat_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient boosting = boosting + descente de gradient\n",
    "#fonctionne de manière très similaire au forêts aléatoires pour la classification.\n",
    "#néanmoins, les arbres ne sont plus appris indépendamment des autres \n",
    "\"\"\"Le BOOSTING est une technique ensembliste qui consiste à agréger des classifieurs (modèles) élaborés \n",
    "séquentiellement sur un échantillon d’apprentissage dont les poids des individus sont corrigés au fur et \n",
    "à mesure. Les classifieurs sont pondérés selon leurs performances\"\"\"\n",
    "gbc40 = GradientBoostingClassifier(n_estimators=50, max_depth=500, learning_rate=0.6)\n",
    "gbc40.fit(feat_train, y_train)\n",
    "gbc40.score(feat_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_grid = {\n",
    "    #\"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    #\"max_depth\":[400, 500],\n",
    "    #\"criterion\": [\"friedman_mse\", \"mae\", \"mse\"],\n",
    "    #'n_estimators' : [50, 70]}\n",
    "\n",
    "#gbc=GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "#gbc_grid = GridSearchCV(gbc, param_grid, cv = 3)\n",
    "\n",
    "#%time GBC = gbc_grid.fit(feat_train, y_train)\n",
    "#print (gbc_grid.best_score_.round(5))\n",
    "#print(gbc_grid.best_params_)\n",
    "\n",
    "#final_model = gbc_grid.best_estimator_\n",
    "\n",
    "#pred_train = final_model.predict(feat_train) \n",
    "#pred_test = final_model.predict(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = MatriceConfusion(gbc40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = gbc40.predict_proba(feat_test)\n",
    "fpr, tpr, th = roc_curve(y_test, score[:, 1])\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "aucf = auc(fpr, tpr)\n",
    "ax.plot(fpr, tpr, label='auc=%1.5f' % aucf)\n",
    "ax.set_title('Courbe ROC - classifieur de sentiments')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#un n-gramme est “une sous-séquence de n éléments construite à partir d’une séquence donnée.” \n",
    "#n-gramme permet de créer un modèle probabiliste pour anticiper le prochain élément d’une suite\n",
    "#padding=\"remplissage\"\n",
    "generated_ngrams = ngrams(word_tokenize(X_train.iloc[0,0]), 3, pad_left=True, pad_right=True)\n",
    "list(generated_ngrams)[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram_range of (1, 2) means unigrams and bigrams\n",
    "#Un 2-gram (or bi-gram) est une séquance de 2 mots : “please turn”, “turn your” \n",
    "pipe2 = make_pipeline(CountVectorizer(ngram_range=(1, 2)),\n",
    "                      TfidfTransformer())\n",
    "pipe2.fit(X_train['sentence'])\n",
    "feat_train2 = pipe2.transform(X_train['sentence'])\n",
    "feat_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_feature_names() - retourne une liste de feature names, classés par leurs indices\n",
    "cl = pipe2.steps[0]\n",
    "cl[1].get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_test2 = pipe2.transform(X_test['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'penalty' : ['l1', 'l2']}\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "lr_grid = GridSearchCV(lr, param_grid, cv = 3)\n",
    "\n",
    "%time LR = lr_grid.fit(feat_train2, y_train)\n",
    "\n",
    "print(lr_grid.best_score_.round(5))\n",
    "print(lr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_test2 = pipe2.transform(X_test['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = lr_grid.best_estimator_\n",
    "\n",
    "pred_train = final_model.predict(feat_train2) \n",
    "pred_test = final_model.predict(feat_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SVD est une technique de factorisation matricielle qui factorise une matrice M dans les trois matrices U,\n",
    "Σ  et V. Très similaire à l'ACP, sauf que la factorisation pour SVD est effectuée sur la matrice de données, tandis que pour PCA, est une technique de factorisation matricielle qui factorise une matrice M dans les trois matrices U, Σ et V.Ceci est très similaire à l'ACP, sauf que la factorisation pour SVD est effectuée sur la matrice de données, \n",
    "pas sur la matrice de covariance.\"\"\"\n",
    "#n_components, dimensions qui doivent être inférieures aux colonnes\n",
    "pipe_svd = make_pipeline(CountVectorizer(), TruncatedSVD(n_components=300))\n",
    "pipe_svd.fit(X_train['sentence'])\n",
    "feat_train_svd = pipe_svd.transform(X_train['sentence'])\n",
    "feat_train_svd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'penalty' : ['l1', 'l2']}\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "lr_grid = GridSearchCV(lr, param_grid, cv = 3)\n",
    "\n",
    "%time LR = lr_grid.fit(feat_train_svd, y_train)\n",
    "\n",
    "print(lr_grid.best_score_.round(5))\n",
    "print(lr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_test_svd = pipe_svd.transform(X_test['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = lr_grid.best_estimator_\n",
    "\n",
    "pred_train = final_model.predict(feat_train_svd) \n",
    "pred_test = final_model.predict(feat_test_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svd_tfidf = make_pipeline(CountVectorizer(),\n",
    "                     TfidfTransformer(),\n",
    "                     TruncatedSVD(n_components=300))\n",
    "pipe_svd_tfidf.fit(X_train['sentence'])\n",
    "feat_train_svd_tfidf = pipe_svd_tfidf.transform(X_train['sentence'])\n",
    "feat_test_svd_tfidf = pipe_svd_tfidf.transform(X_test['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'penalty' : ['l1', 'l2']}\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "lr_grid = GridSearchCV(lr, param_grid, cv = 3)\n",
    "\n",
    "%time LR = lr_grid.fit(feat_train_svd_tfidf, y_train)\n",
    "\n",
    "print(lr_grid.best_score_.round(5))\n",
    "print(lr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = lr_grid.best_estimator_\n",
    "\n",
    "pred_train = final_model.predict(feat_train_svd_tfidf) \n",
    "pred_test = final_model.predict(feat_test_svd_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svd_tfidf = make_pipeline(CountVectorizer(),\n",
    "                     TfidfTransformer(),\n",
    "                     TruncatedSVD(n_components=300))\n",
    "pipe_svd_tfidf.fit(X_train['sentence'])\n",
    "feat_train_svd_tfidf = pipe_svd_tfidf.transform(X_train['sentence'])\n",
    "\n",
    "clf_svd_tfidf = LogisticRegression()\n",
    "clf_svd_tfidf.fit(feat_train_svd_tfidf, y_train)\n",
    "\n",
    "feat_test_svd_tfidf = pipe_svd_tfidf.transform(X_test['sentence'])\n",
    "clf_svd_tfidf.score(feat_test_svd_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deacc=True, enlève les ponctuations\n",
    "from gensim.utils import tokenize\n",
    "sentance = [list(tokenize(s, deacc=True, lower=True)) for s in X_train['sentence']]\n",
    "sentance[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec, réductions des dimensions\n",
    "#paramètre : https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentance, size=300, window=20,\n",
    "                          min_count=2, workers=1, iter=100)\n",
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = model.wv.vocab\n",
    "list(vocab)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('trained_word2vec.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv['fabuleux'].shape, model.wv['fabuleux'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.wv['rrrrrrrr']\n",
    "except KeyError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def get_vect(word, model):\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except KeyError:\n",
    "        return numpy.zeros((model.vector_size,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sum_vectors(phrase, model):\n",
    "    return sum(get_vect(w, model) for w in phrase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_features(X, model):\n",
    "    feats = numpy.vstack([sum_vectors(p, model) for p in X])\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wv_train_feat = word2vec_features(X_train[\"sentence\"], model)\n",
    "#wv_train_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def get_vect(word, model):\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except KeyError:\n",
    "        return numpy.zeros((model.vector_size,))\n",
    "\n",
    "def sum_vectors(phrase, model):\n",
    "    return sum(get_vect(w, model) for w in phrase)\n",
    "\n",
    "def word2vec_features(X, model):\n",
    "    feats = numpy.vstack([sum_vectors(p, model) for p in X])\n",
    "    return feats\n",
    "\n",
    "wv_train_feat = word2vec_features(X_train[\"sentence\"], model)\n",
    "wv_train_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vect(word, model):\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except KeyError:\n",
    "        return numpy.zeros((model.vector_size,))\n",
    "\n",
    "def sum_vectors(phrase, model):\n",
    "    return sum(get_vect(w, model) for w in phrase)\n",
    "\n",
    "def word2vec_features(X, model):\n",
    "    feats = numpy.vstack([sum_vectors(p, model) for p in X])\n",
    "    return feats\n",
    "\n",
    "wv_train_feat = word2vec_features(X_train[\"sentence\"], model)\n",
    "wv_train_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfwv = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "clfwv.fit(wv_train_feat, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_test_feat = word2vec_features(X_test[\"sentence\"], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfwv.score(wv_test_feat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(sorted(model.wv.vocab))\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ['exceptionnel', 'personnel', words[3], words[4], words[5]]\n",
    "rows = []\n",
    "for w in subset:\n",
    "    for ww in subset:\n",
    "        rows.append(dict(w1=w, w2=ww, d=model.wv.similarity(w, ww)))\n",
    "import pandas\n",
    "pandas.DataFrame(rows).pivot(\"w1\", \"w2\", \"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y variable dépendante et X variables indépendantes\n",
    "#X = df.iloc[:, df.columns !='polaritecomments'].values\n",
    "#y = df.iloc[:, 5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "#print (X_train.shape,y_train.shape)\n",
    "#print (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = StandardScaler()\n",
    "#X_train = sc.fit_transform(X_train)\n",
    "#X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.hist(figsize=(13,50),color='blue',bins=40,layout=(8,3))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_grid = {'max_depth' : [3, 10, 15],\n",
    "#    'random_state': [0, 42],\n",
    "#             'n_estimators' : [10, 50]}\n",
    "#grid = GridSearchCV( RandomForestClassifier(), param_grid)\n",
    "\n",
    "#%time grid.fit(X_train, y_train)\n",
    "#print(grid.best_params_)\n",
    "\n",
    "#model = grid.best_estimator_\n",
    "#yfit = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights = (y == 0).sum() / (1.0 * (y == 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_score(model,X_train,X_test,y_train,y_test):\n",
    "    #model.fit(X_train,y_train)\n",
    "    #return model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3)\n",
    "\n",
    "LR = LogisticRegression()\n",
    "SVM = SVC()\n",
    "NB = nb.BernoulliNB()\n",
    "DTC = DecisionTreeClassifier(max_depth = 5, random_state= 42)\n",
    "RF = RandomForestClassifier(max_depth = 5, n_estimators= 10, random_state= 42)\n",
    "XGBC = XGBClassifier(max_depth = 5, scale_pos_weight = weights, \\\n",
    "                n_jobs = 4)\n",
    "\n",
    "logis=list()\n",
    "svm=list()\n",
    "nb=list()\n",
    "arb=list()\n",
    "rfl=list()\n",
    "clf=list()\n",
    "\n",
    "for index_train, index_test in kf.split(d_model):\n",
    "    X_train, X_test, y_train, y_test = X[index_train], X[index_test], y[index_train], y[index_test]\n",
    "    logis.append(get_score(LR, X_train, X_test, y_train, y_test))\n",
    "    svm.append(get_score(SVM, X_train, X_test, y_train, y_test))\n",
    "    nb.append(get_score(NB, X_train, X_test, y_train, y_test))\n",
    "    arb.append(get_score(DTC, X_train, X_test, y_train, y_test))\n",
    "    rfl.append(get_score(RF, X_train, X_test, y_train, y_test))\n",
    "    clf.append(get_score(XGBC, X_train, X_test, y_train, y_test))\n",
    "    \n",
    "print(np.mean(logis))\n",
    "print(np.mean(svm))\n",
    "print(np.mean(arb))\n",
    "print(np.mean(rfl))\n",
    "print(np.mean(clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y variable dépendante et X variables indépendantes\n",
    "X = d_model.iloc[:, d_model.columns !='isFraud']\n",
    "Y = d_model.iloc[:, 5]\n",
    "\n",
    "X_t = pd.DataFrame(X_train)\n",
    "feature_importances = pd.DataFrame(RF.feature_importances_, index = X.columns,columns=['importance']).sort_values('importance',ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X.columns\n",
    "importances=RF.feature_importances_\n",
    "indices=np.argsort(importances)\n",
    "plt.barh(range(len(indices)), importances[indices])\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('importance du score');\n",
    "plt.ylabel('variables indépendantes');\n",
    "plt.title('Ordre d\\'importance des variables indépendantes');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatriceConfusion (model):\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.matshow(cm)\n",
    "    plt.title('Matrice de confusion', y=1.12)\n",
    "    plt.colorbar()\n",
    "    print(cm)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = MatriceConfusion(LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NaiveBayes = muticlasses\n",
    "#SVM = multiclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

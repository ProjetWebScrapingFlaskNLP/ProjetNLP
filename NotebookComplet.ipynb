{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Machine learning</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import plot_importance, to_graphviz\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, auc, precision_score, recall_score\n",
    "from sklearn import feature_extraction, model_selection, svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.util import ngrams\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\utilisateur\\Documents\\Projet\\ProjetNLP\\DatasetMachineLearning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#création jeux train et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['sentence']], df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44133, 16659)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"CountVectorizer de Scikit-learn est utilisé pour transformer un corpus de mots en \n",
    "vecteurs/occurence des mots \"\"\"\n",
    "#Tf signifie term-frequency tandis que tfidf signifie inverse document-frequency\n",
    "#IDF(word) = Log((Total number of documents)/(Number of documents containing the word))\n",
    "pipe = make_pipeline(CountVectorizer(), TfidfTransformer())\n",
    "pipe.fit(X_train['sentence'])\n",
    "feat_train = pipe.transform(X_train['sentence'])\n",
    "feat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#minimum et maximum pour feat_train\n",
    "feat_train.min(), feat_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14711, 16659)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transform sans fit sur le jeu test\n",
    "feat_test = pipe.transform(X_test['sentence'])\n",
    "feat_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 39.2 s\n",
      "0.9102\n",
      "{'criterion': 'gini', 'max_depth': 60}\n"
     ]
    }
   ],
   "source": [
    "#définir les paramètres\n",
    "#le paramètre max_depth est un seuil sur la profondeur maximale de l’arbre\n",
    "\"\"\"L'indice de diversité de gini : probabilité pour chaque élément d'être choisi multipliée \n",
    "par la probabilité qu'il soit mal classé\"\"\"\n",
    "\"\"\"L'entropie au lieu d'utiliser les probabilités simples applique le log2 des probabilités\"\"\"\n",
    "param_grid = {'max_depth' : [40, 60], \n",
    "              'criterion' : ['gini', 'entropy']}\n",
    "             \n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "dtc_grid = GridSearchCV(dtc, param_grid, cv=3)\n",
    " \n",
    "#entrainer le modèle à partir de Grid Search\n",
    "%time CV_dtc = dtc_grid.fit(feat_train, y_train)\n",
    "\n",
    "print(dtc_grid.best_score_.round(5))\n",
    "print(dtc_grid.best_params_)\n",
    "\n",
    "final_model = dtc_grid.best_estimator_\n",
    "\n",
    "pred_train = final_model.predict(feat_train) \n",
    "pred_test = final_model.predict(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96     22388\n",
      "           1       0.99      0.93      0.96     21745\n",
      "\n",
      "    accuracy                           0.96     44133\n",
      "   macro avg       0.96      0.96      0.96     44133\n",
      "weighted avg       0.96      0.96      0.96     44133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92      7511\n",
      "           1       0.94      0.88      0.91      7200\n",
      "\n",
      "    accuracy                           0.91     14711\n",
      "   macro avg       0.92      0.91      0.91     14711\n",
      "weighted avg       0.92      0.91      0.91     14711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatriceConfusion(model):\n",
    "    y_pred = model.predict(feat_test)\n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, pred_test)\n",
    "    plt.matshow(cm)\n",
    "    plt.title('Matrice de confusion', y=1.12)\n",
    "    plt.colorbar()\n",
    "    print(cm)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b35df6024aa6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMatriceConfusion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCV_dtc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-22a8238416a7>\u001b[0m in \u001b[0;36mMatriceConfusion\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Matrice de confusion'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "y_prediction = MatriceConfusion(CV_dtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = final_model.predict_proba(feat_test)\n",
    "fpr, tpr, th = roc_curve(y_test, score[:, 1])\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "aucf = auc(fpr, tpr)\n",
    "ax.plot(fpr, tpr, label='auc=%1.5f' % aucf)\n",
    "ax.set_title('Courbe ROC - classifieur de sentiments')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"n_estimators = le nombre d'arbres dans la fôrét\"\"\"\n",
    "\"\"\"Le Bootstrapping est un procédé qui permet d’augmenter artificiellement le nombre d’observation d’un \n",
    "échantillon de données sans pour autant modifier la distribution des variables présentes dans le jeu de \n",
    "données. Le principe est simple, on dispose d’un jeu de données contenant n observations, pour créer un \n",
    "échantillon de taille n on tire avec remise n observations parmi le jeu de données original\"\"\"\n",
    "param_grid = {#'max_depth' : [40, 60],\n",
    "             #'n_estimators' : [30, 50, 200],\n",
    "              'bootstrap' : [True, False],\n",
    "              'criterion' : ['gini', 'entropy']}\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rfc_grid = GridSearchCV(rfc, param_grid, cv = 3)\n",
    "\n",
    "%time CV_rfc=rfc_grid.fit(feat_train, y_train)\n",
    "\n",
    "print(rfc_grid.best_score_.round(5))\n",
    "print(rfc_grid.best_params_)\n",
    "\n",
    "final_model = rfc_grid.best_estimator_\n",
    "\n",
    "pred_train = final_model.predict(feat_train) \n",
    "pred_test = final_model.predict(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = MatriceConfusion(CV_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = final_model.predict_proba(feat_test)\n",
    "fpr, tpr, th = roc_curve(y_test, score[:, 1])\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "aucf = auc(fpr, tpr)\n",
    "ax.plot(fpr, tpr, label='auc=%1.5f' % aucf)\n",
    "ax.set_title('Courbe ROC - classifieur de sentiments')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"L1 : Lasso et L2 : Ridge, pénalité, c’est une modification qu’on apporte à la fonction de coût \n",
    "afin de maîtriser l’arbitrage entre biais vs variance\"\"\"\n",
    "\"\"\"la fonction de coût est définie comme le carré de la différence entre la valeur prévue et la \n",
    "valeur réelle en fonction de l'intrant\"\"\"\n",
    "param_grid = {'penalty' : ['l1', 'l2'],\n",
    "              'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "lr_grid = GridSearchCV(lr, param_grid, cv = 3)\n",
    "\n",
    "%time CV_lr=lr_grid.fit(feat_train, y_train)\n",
    "\n",
    "print(lr_grid.best_score_.round(5))\n",
    "print(lr_grid.best_params_)\n",
    "\n",
    "final_model = lr_grid.best_estimator_\n",
    "\n",
    "pred_train = final_model.predict(feat_train) \n",
    "pred_test = final_model.predict(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = MatriceConfusion(CV_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = CV_lr.decision_function(feat_test)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Le mot Bagging est une contraction de Bootstrap Aggregation : sélection de sous-ensembles aléatoires de\n",
    "l'ensemble des données d'origine, puis agrégation des prédictions individuelles pour former une prédiction\n",
    "finale\"\"\" \n",
    "naivebayes = nb.BernoulliNB()\n",
    "modelB = BC(base_estimator = naivebayes, n_estimators = 1000, warm_start = True, bootstrap_features=True, max_samples=0.9)\n",
    "modelB_fit = modelB.fit(feat_train, y_train)\n",
    "modelB_fit.score(feat_test,y_test)\n",
    "pred_train = modelB.predict(feat_train)\n",
    "pred_test = modelB.predict(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = MatriceConfusion(modelB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = modelB_fit.predict_proba(feat_test)\n",
    "fpr, tpr, th = roc_curve(y_test, score[:, 1])\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "aucf = auc(fpr, tpr)\n",
    "ax.plot(fpr, tpr, label='auc=%1.5f' % aucf)\n",
    "ax.set_title('Courbe ROC - classifieur de sentiments')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {#'C': [10, 50, 100,200],\n",
    "              #'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf','sigmoid']}\n",
    "\n",
    "svm = svm.SVC(random_state=42)\n",
    "\n",
    "clf_grid = GridSearchCV(svm, param_grid, cv=3)\n",
    "\n",
    "svm = clf_grid.fit(feat_train, y_train)\n",
    "\n",
    "print(clf_grid.best_score_.round(2))\n",
    "print(clf_grid.best_params_)\n",
    "\n",
    "final_model = clf_grid.best_estimator_\n",
    "\n",
    "pred_train = final_model.predict(feat_train) \n",
    "pred_test = final_model.predict(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = MatriceConfusion(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = svm.decision_function(feat_test)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    #\"learning_rate\": [0.05,0.01, 0.1],\n",
    "    #\"max_depth\":[20, 30, 50],\n",
    "    #\"criterion\": [\"friedman_mse\", \"mae\", \"mse\"],\n",
    "    'n_estimators' : [30, 50, 70]}\n",
    "\n",
    "gbc=GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "gbc_grid = GridSearchCV(gbc, param_grid, cv = 3)\n",
    "\n",
    "%time GBC = gbc_grid.fit(feat_train, y_train)\n",
    "print (gbc_grid.best_score_.round(5))\n",
    "print(gbc_grid.best_params_)\n",
    "\n",
    "final_model = gbc_grid.best_estimator_\n",
    "\n",
    "pred_train = final_model.predict(feat_train) \n",
    "pred_test = final_model.predict(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = MatriceConfusion(GBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = gbc_grid.predict_proba(feat_test)\n",
    "fpr, tpr, th = roc_curve(y_test, score[:, 1])\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "aucf = auc(fpr, tpr)\n",
    "ax.plot(fpr, tpr, label='auc=%1.5f' % aucf)\n",
    "ax.set_title('Courbe ROC - classifieur de sentiments')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#un n-gramme est “une sous-séquence de n éléments construite à partir d’une séquence donnée” \n",
    "#n-gramme permet de créer un modèle probabiliste pour anticiper le prochain élément d’une suite\n",
    "generated_ngrams = ngrams(word_tokenize(X_train.iloc[0,0]), 3, pad_left=True, pad_right=True)\n",
    "list(generated_ngrams)[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Un 2-gram (or bi-gram) est une séquance de 2 mots : “please turn”, “turn your”\n",
    "pipe2 = make_pipeline(CountVectorizer(ngram_range=(1, 2)),\n",
    "                      TfidfTransformer())\n",
    "pipe2.fit(X_train['sentence'])\n",
    "feat_train2 = pipe2.transform(X_train['sentence'])\n",
    "feat_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_feature_names() - retourne une liste de feature names, classés par leurs indices\n",
    "cl = pipe2.steps[0]\n",
    "cl[1].get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_test2 = pipe2.transform(X_test['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = LogisticRegression()\n",
    "clf2.fit(feat_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2.score(feat_test2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SVD est une technique de factorisation matricielle qui factorise une matrice M dans les trois matrices U,\n",
    "Σ  et V. Très similaire à l'ACP, sauf que la factorisation pour SVD est effectuée sur la matrice de données, tandis que pour PCA, est une technique de factorisation matricielle qui factorise une matrice M dans les trois matrices U, Σ et V.Ceci est très similaire à l'ACP, sauf que la factorisation pour SVD est effectuée sur la matrice de données, \n",
    "pas sur la matrice de covariance.\"\"\"\n",
    "#n_components, dimensions qui doivent être inférieures aux colonnes\n",
    "pipe_svd = make_pipeline(CountVectorizer(), TruncatedSVD(n_components=300))\n",
    "pipe_svd.fit(X_train['sentence'])\n",
    "feat_train_svd = pipe_svd.transform(X_train['sentence'])\n",
    "feat_train_svd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svd = RandomForestClassifier(n_estimators=50, max_depth=40, random_state=42)\n",
    "clf_svd.fit(feat_train_svd, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_test_svd = pipe_svd.transform(X_test['sentence'])\n",
    "clf_svd.score(feat_test_svd, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_svd = LogisticRegression()\n",
    "lr_svd.fit(feat_train_svd, y_train)\n",
    "lr_svd.score(feat_test_svd, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svd_tfidf = make_pipeline(CountVectorizer(),\n",
    "                     TfidfTransformer(),\n",
    "                     TruncatedSVD(n_components=300))\n",
    "pipe_svd_tfidf.fit(X_train['sentence'])\n",
    "feat_train_svd_tfidf = pipe_svd_tfidf.transform(X_train['sentence'])\n",
    "\n",
    "clf_svd_tfidf = LogisticRegression()\n",
    "clf_svd_tfidf.fit(feat_train_svd_tfidf, y_train)\n",
    "\n",
    "feat_test_svd_tfidf = pipe_svd_tfidf.transform(X_test['sentence'])\n",
    "clf_svd_tfidf.score(feat_test_svd_tfidf, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

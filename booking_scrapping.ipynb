{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, ElementClickInterceptedException, ElementNotInteractableException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "import pprint\n",
    "import re\n",
    "import datetime\n",
    "import locale\n",
    "from selenium.webdriver.chrome.options import Options \n",
    "import chromedriver_binary\n",
    "from stopit import SignalTimeout as Timeout\n",
    "from stopit import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fr_FR.UTF-8'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# necessary to get french date\n",
    "locale.setlocale(locale.LC_ALL, 'fr_FR.UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# headless mode\n",
    "chrome_options = Options()  \n",
    "\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "# chrome_options.add_argument(\"--headless\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_persistent(backup_link, cols, new_data):\n",
    "    \"\"\"\n",
    "    save data to csv file\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        backup = pd.read_csv(backup_link)\n",
    "    except FileNotFoundError:\n",
    "        backup = pd.DataFrame([], columns=cols)\n",
    "        \n",
    "    # if new_data isn't a dataframe, we create a DataFrame from it\n",
    "    if not isinstance(new_data, pd.core.frame.DataFrame):\n",
    "        try:\n",
    "            new_data = pd.DataFrame(new_data, columns=cols)\n",
    "        except:\n",
    "            raise\n",
    "    \n",
    "    new_backup = pd.concat([backup, new_data])\n",
    "    new_backup.to_csv(backup_link, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accessibility(browser):\n",
    "    \"\"\"\n",
    "    if cookie banner and robot detection are present, remove them\n",
    "    \"\"\"\n",
    "    \n",
    "    # if the are you human popup appears, remove it\n",
    "    try :\n",
    "        robot_detection = browser.find_element_by_id('botdetect_abu_nip__overlay')\n",
    "        return False\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    \n",
    "    # remove cookie banner because it takes almost 50% height of the page\n",
    "    try:\n",
    "        cookie_banner = browser.find_element_by_css_selector('#cookie_warning button')\n",
    "        cookie_banner.click()\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_page(url):\n",
    "    \"\"\"\n",
    "    reload webpage if network not available\n",
    "    \"\"\"\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            browser = webdriver.Chrome(options=chrome_options)\n",
    "            browser.get(url)\n",
    "            time.sleep(3) # let's the DOM load\n",
    "            \n",
    "            logo = browser.find_element_by_css_selector('#logo_no_globe_new_logo') # if it can't get the logo, it means the page isn't loaded\n",
    "            robot_detection = accessibility(browser)\n",
    "    \n",
    "            if robot_detection :\n",
    "                break\n",
    "            else :\n",
    "                browser.close()\n",
    "        except:\n",
    "            browser.close() # reload the page\n",
    "    \n",
    "    return browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to get the hotel links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hotels_links_by_page(browser, query):\n",
    "    \"\"\"\n",
    "    get all hotel links inside a webpage\n",
    "    \"\"\"\n",
    "    \n",
    "    # we can compare it to a previous backup if we have it\n",
    "    try:\n",
    "        backup_links = pd.read_csv(f'backup_hotel_links_{query}.csv')\n",
    "    except:\n",
    "        backup_links = None\n",
    "        print('no backup file')\n",
    "    \n",
    "    hotel_links_array = []\n",
    "\n",
    "    # get link for each hotel in the page\n",
    "    for hotel in browser.find_elements_by_css_selector('#hotellist_inner .sr_item'):\n",
    "        \n",
    "        try:\n",
    "            # we only want hotel with reviews\n",
    "            has_rating = hotel.find_elements_by_css_selector('.bui-review-score__badge')\n",
    "            \n",
    "            if len(has_rating) > 0 :\n",
    "                link = hotel.find_element_by_css_selector('h3 .hotel_name_link').get_attribute('href')\n",
    "                \n",
    "                existing_link = []\n",
    "                \n",
    "                if backup_links is not None:\n",
    "                    existing_link = backup_links.loc[backup_links['link'] == link]\n",
    "                \n",
    "                if len(existing_link) == 0: # if not in database or if the backup file doesn't exist\n",
    "                    # we want the short version of the link to save space in the csv\n",
    "                    pattern = re.compile(r'(.+)\\?')\n",
    "                    result = pattern.match(link)\n",
    "                    short_link = result.group(1)\n",
    "                    hotel_links_array.append([short_link, 0])\n",
    "                    \n",
    "        except StaleElementReferenceException as e:\n",
    "            print(e)\n",
    "       \n",
    "    # persistent data\n",
    "    make_data_persistent(f'backup_hotel_links_{query}.csv', ['link', 'has_been_scrapped'], hotel_links_array)\n",
    "    \n",
    "    return hotel_links_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_hotel_links(browser, query):\n",
    "    \"\"\"\n",
    "    loop through all results pages to get hotel links to scrap\n",
    "    \"\"\"\n",
    "    \n",
    "    all_links = []\n",
    "    \n",
    "    while True:\n",
    "        time.sleep(2) # to prevent stale elements\n",
    "        try :\n",
    "            all_links.append(get_hotels_links_by_page(browser, query))\n",
    "            next_btn = browser.find_element_by_css_selector('.bui-pagination__next-arrow:not(.bui-pagination__item--disabled) .bui-pagination__link')\n",
    "            next_btn.click()\n",
    "        except NoSuchElementException:\n",
    "            print('no more results') # no more results\n",
    "            break\n",
    "        except StaleElementReferenceException as e:\n",
    "            print(e)\n",
    "            \n",
    "    # flatten the list of all links\n",
    "    all_links = [link[0] for links_by_page in all_links for link in links_by_page]\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to fetch comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_for_comment_item(review, col):\n",
    "    \"\"\"\n",
    "    optimize the process of collecting the values of reviews elements (title, name, rating, etc..)\n",
    "    \"\"\"\n",
    "    \n",
    "    items = {\n",
    "        'column': ['nom', 'pays', 'favorite', 'date', 'titre', 'bons_points', 'mauvais_points', 'note'],\n",
    "        'css_selector' : ['.bui-avatar-block__title', '.bui-avatar-block__subtitle', '.c-review-block__badge', '.c-review-block__date', '.c-review-block__title', '.c-review__row:not(.lalala) .c-review__body', '.lalala .c-review__body', '.bui-review-score__badge'],\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # we get the index of the column we are dealing with, to get the related css\n",
    "        col_idx = items['column'].index(col)\n",
    "        css = items['css_selector'][col_idx]\n",
    "        \n",
    "        # we get the value related to the column\n",
    "        item = review.find_element_by_css_selector(css).text\n",
    "            \n",
    "        # date info is shown like this : 'Commentaire envoyÃ© le DD/MM/YYYY', \n",
    "        # we only need the date inside the text\n",
    "        if col == 'date':\n",
    "            try: \n",
    "                pattern = re.compile(r'\\d{1,2}\\s\\w+\\s\\w{4}')\n",
    "                result = pattern.search(item)\n",
    "                date_str = result.group()\n",
    "                item = datetime.datetime.strptime(date_str, \"%d %B %Y\")\n",
    "            except AttributeError:\n",
    "                item = 'None'\n",
    "                \n",
    "        # in case we are dealing with the Choix de l'utilisateur / favorite column\n",
    "        # we don't need the text, if favorite element present in the block => 1 otherwise 0\n",
    "        if col == 'favorite' and item:\n",
    "            item = 1\n",
    "                \n",
    "        return item\n",
    "    \n",
    "    except NoSuchElementException: # in case the item can't be fetched\n",
    "         if col == 'favorite':\n",
    "            return 0\n",
    "         else :\n",
    "            return 'None' # other exceptions will lead to the closing of the current browser, so we are bubbbling them up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment_data(review, etablissement, cols):\n",
    "    \"\"\"\n",
    "    get the comment data\n",
    "    \"\"\"\n",
    "    \n",
    "    new_row = []\n",
    "    \n",
    "    # we can get the first 8 cells from the review block itself\n",
    "    for col in cols[:8]:\n",
    "        new_row.append(get_value_for_comment_item(review, col))\n",
    "    \n",
    "    # we collect also the data about the accomodation\n",
    "    new_row.append(etablissement['type'])\n",
    "    new_row.append(etablissement['lieu'])\n",
    "    new_row.append(etablissement['note'])\n",
    "    \n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(browser, etablissement, query, link):\n",
    "    \"\"\"\n",
    "    comments can be displayed on several pages. we want max 300 reviews per hotel\n",
    "    \"\"\"\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    # create new dataframe to save the comments into a backup csv file\n",
    "    cols = ['nom', 'pays', 'favorite', 'date', 'titre', 'bons_points', 'mauvais_points', 'note', 'type_etablissement', 'lieu', 'note_etablissement']\n",
    "    data = pd.DataFrame([], columns=cols)\n",
    "    \n",
    "    while True:\n",
    "        time.sleep(2) # wait till booking get the following comments\n",
    "        \n",
    "        # we get each review and call the script to get its content\n",
    "        for review in browser.find_elements_by_css_selector('.review_list .review_list_new_item_block'): \n",
    "        \n",
    "            # scroll to the review\n",
    "            browser.execute_script('arguments[0].scrollIntoView({behavior: \"smooth\", block: \"end\", inline: \"nearest\"});', review)\n",
    "        \n",
    "            # we add the content of the review\n",
    "            data.loc[len(data.index)] = get_comment_data(review, etablissement, cols)\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "        # when we are done with a section of comments, we check if we have at least 300 comments\n",
    "        if count >= 300:\n",
    "            break \n",
    "            \n",
    "        # otherwise, we open a new comments panel\n",
    "        try :\n",
    "            next_btn = browser.find_element_by_css_selector('#review_list_score_container .bui-pagination__next-arrow:not(.bui-pagination__item--disabled) a')\n",
    "            next_btn.click()\n",
    "        except NoSuchElementException:\n",
    "            print('no more comments to load')\n",
    "            break\n",
    "    \n",
    "    # we save the scrapped comments in a backup csv file\n",
    "    make_data_persistent(f'backup_booking_{query}.csv', cols, data)\n",
    "    \n",
    "    # update the hotel_links list of the query, so when we have to do scrap again we can resume at the right spot\n",
    "    try:\n",
    "        backup_links = pd.read_csv(f'backup_hotel_links_{query}.csv')\n",
    "        mask = backup_links['link'] == link\n",
    "        backup_links.loc[mask, 'has_been_scrapped'] = 1\n",
    "        backup_links.to_csv(f'backup_hotel_links_{query}.csv', index=False)\n",
    "    except FileNotFoundError:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_comments_panel(browser):\n",
    "    \"\"\"\n",
    "    get hotel page, open French comments section and return the hotel and browser elements to the main caller\n",
    "    \"\"\"\n",
    "    \n",
    "    # open reviews panel\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        btn_cmt = browser.find_element_by_id('show_reviews_tab')\n",
    "        btn_cmt.click()\n",
    "    except NoSuchElementException as e:\n",
    "        print(e, 'there is no review')\n",
    "        return False\n",
    "    except ElementClickInterceptedException as e:\n",
    "        print(e, 'already open') # if the webpage has already been visited by us\n",
    "        \n",
    "    #get only french reviews\n",
    "    time.sleep(2) # wait a little bit till the checkbox is available\n",
    "    try:\n",
    "        btn_french = browser.find_element_by_css_selector('.language_filter_checkbox[value=\"fr\"] + span')\n",
    "        btn_french.click()\n",
    "    except NoSuchElementException as e:\n",
    "        print(e, 'there is no french review')\n",
    "        return False\n",
    "    \n",
    "    # get only bad comments - uncomment if you want only bad comments\n",
    "    # try:\n",
    "        # select = Select(browser.find_element_by_id('review_score_filter'))\n",
    "        # select.select_by_index(4)\n",
    "    # except NoSuchElementException as e:\n",
    "        # print('no bad comment')\n",
    "\n",
    "    # it has to take into account the language change\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # get info about accomodation\n",
    "    try:\n",
    "        etablissement = {\n",
    "            'nom': browser.find_element_by_css_selector('.hp__hotel-name').text,\n",
    "            'type' : browser.find_element_by_css_selector('.hp__hotel-name span').text,\n",
    "            'note': browser.find_element_by_css_selector('.reviewFloater .bui-review-score__badge').get_attribute('innerHTML'), # sometimes it is hidden\n",
    "            'lieu' : browser.find_element_by_css_selector('.sb-destination__input').get_attribute(\"value\")\n",
    "        }\n",
    "    except NoSuchElementException as e:\n",
    "        etablissement = {\n",
    "            'nom': 'None',\n",
    "            'type' : 'None',\n",
    "            'note': 'None', # sometimes it is hidden\n",
    "            'lieu' : 'None'\n",
    "        }\n",
    "        \n",
    "    return etablissement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_booking(query):\n",
    "    \"\"\"\n",
    "    1 - get all hotel links for one city\n",
    "    2 - for each link, get at most 300 comments\n",
    "    \"\"\"\n",
    "    \n",
    "    # we reload the page until we can access it\n",
    "    browser = reload_page(\"https://booking.com\")\n",
    "    \n",
    "    # send query value\n",
    "    search_input = browser.find_element_by_id('ss')\n",
    "    search_input.send_keys(query)\n",
    "    \n",
    "    # btn submit\n",
    "    btn_submit = browser.find_element_by_class_name('sb-searchbox__button')\n",
    "    btn_submit.click()\n",
    "    \n",
    "    # if it is not the first time, we scrap, we resume the process with the corresponding csv file\n",
    "    try :\n",
    "        hotel_links = pd.read_csv(f'backup_hotel_links_{query}.csv')\n",
    "        mask = hotel_links['has_been_scrapped'] == 0\n",
    "        all_links = hotel_links.loc[mask, 'link']\n",
    "    except Exception as e: \n",
    "        print(e, 'the backup file doesnt exist')\n",
    "        all_links = get_all_hotel_links(browser, query)\n",
    "        \n",
    "    # quit Chrome\n",
    "    browser.close()\n",
    "    print('got all hotel links')\n",
    "    \n",
    "    #get comments for each hotel\n",
    "    for link in all_links:\n",
    "        \n",
    "        # we reload the page until we can access it\n",
    "        new_browser = reload_page(link)\n",
    "            \n",
    "        try: # 4 min to get all comments, otherwise we go to the next link\n",
    "            with Timeout(240.0) as timeout_ctx:\n",
    "                \n",
    "                etablissement = open_comments_panel(new_browser)\n",
    "                if etablissement: # we fetch comment only if we can open the comments panel\n",
    "                    get_comments(new_browser, etablissement, query, link)\n",
    "        except TimeoutException as e:\n",
    "            print(e, 'timeout')\n",
    "        except Exception as e: # all exceptions not catched in subprocesses will be dealt here\n",
    "            print(e, 'unexpected error')\n",
    "        finally:\n",
    "            new_browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['Paris', 'Nice', 'Toulouse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with multiprocessing.Pool() as pool:\n",
    "    pool.map(connect_to_booking, cities)\n",
    "\n",
    "# stop manually when you achieve the number you want\n",
    "# we can put a fixed number for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets\n",
    "def merge_datasets(cities):\n",
    "    list_datasets = []\n",
    "\n",
    "    for city in cities :\n",
    "        dataset = pd.read_csv(f'backup_booking_{city}.csv')\n",
    "        list_datasets.append(dataset)\n",
    "\n",
    "\n",
    "    comments = pd.concat(list_datasets, axis=0)\n",
    "    comments.to_csv('booking_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_datasets(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to combine previous scraping job and merge with negative comments collected later on\n",
    "# df = pd.read_csv('positive_comments.csv')\n",
    "# df.head()\n",
    "\n",
    "# we delete empty rows\n",
    "# df = df.dropna(how='all')\n",
    "# data = df[:30000] # and get only 30k to balance comments\n",
    "\n",
    "# neg = pd.read_csv('negative_comments.csv')\n",
    "# comments = pd.concat([data, neg], axis=0)\n",
    "# shuffle comments\n",
    "# comments = comments.sample(frac=1)\n",
    "# comments.to_csv('booking_comments.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet NLP - Groupe 1 - Nohossat, Valérie, Williams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consignes\n",
    "\n",
    "Vous venez d'ouvrir un hôtel. Comme vous n'êtes pas sûr de la qualité de votre établissement, vous permettez aux personnes de poster des commentaires mais pas de mettre de note. Cependant, vous voulez quand même déterminer si le commentaire est positif ou négatif.  \n",
    "\n",
    "Pour cela, vous allez scrapper des commentaires sur booking et leur note associée afin de faire tourner un algorithme de classification pour faire des prédictions sur vos propres commentaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "https://projetnlp.herokuapp.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Récuperation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Site de référence : https://www.booking.com\n",
    "\n",
    "### Contraintes du scraping\n",
    "\n",
    "On a récupéré les commentaires des hôtels de plusieurs villes françaises telles que **Paris, Marseille, Lyon, etc...**  \n",
    "\n",
    "Booking ne permet pas d'ouvrir la page d'un hôtel dans le même onglet donc on a procédé en 2 temps :\n",
    "\n",
    "- Récupération des liens vers les hôtels avec des commentaires (on ignore ceux qui n'ont aucun commentaire)\n",
    "- Récupération des commentaires pour chaque hôtel présélectionné\n",
    "\n",
    "Cette démarche nous permet de reprendre le scraping en cas de crash puisque les liens vers les hôtels sont sauvegardés avant de passer à la deuxième étape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"1000px\" src='booking.gif' alt='booking_website'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données récupérées\n",
    "\n",
    "Pour chaque hôtel, on récupère au maximum 300 commentaires et les informations suivantes:\n",
    "\n",
    "- **NOM** : nom du client ayant laissé le commentaire\n",
    "- **PAYS** : pays de provenance du client\n",
    "- **FAVORITE** : est-ce que le client a marqué l'établissement parmi ces favoris ?\n",
    "- **DATE** : date d'envoi du commentaire\n",
    "- **TITRE** : titre du commentaire laissé par le client\n",
    "- **BONS POINTS** : les aspects positifs de l'expérience\n",
    "- **MAUVAIS POINTS** : les aspects négatifs de l'expérience\n",
    "- **NOTE** : la note laissée par le client\n",
    "- **TYPE ETABLISSEMENT**: le type de l'établissement (Appartement, Hôtel, etc..)\n",
    "- **LIEU** : Ville de l'établissement\n",
    "- **NOTE ETABLISSEMENT** : note moyenne laissée par l'ensemble des commentateurs\n",
    "\n",
    "<img width=\"600px\" src='commentaire_booking.png' alt='booking_commentaire'>\n",
    "\n",
    "### Résilience du scraping\n",
    "\n",
    "Plusieurs actions ont été mises en place pour rendre le scraping résilient: \n",
    "\n",
    "- relance de la page en cas d'apparition du pop-up 'Etes-vous humain?'\n",
    "- timeout de 4min pour récupérer les 300 commentaires par hôtel\n",
    "- multiprocessing (1 process par ville)\n",
    "- backup après chaque commentaire récupéré\n",
    "\n",
    "### Limites\n",
    "\n",
    "Lors du preprocessing, on s'est rendu compte qu'il y avait une prédominance des commentaires positifs. Le scraping a donc du être réalisé une deuxième fois pour récupérer seulement les commentaires négatifs.\n",
    "\n",
    "\n",
    "### Résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Récupération de {df.shape[0]} commentaires')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Analyse des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nohossat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nohossat/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from stop_words import get_stop_words\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import ComplementNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score, auc\n",
    "\n",
    "# plt.rcParams['text.color'] = 'black'\n",
    "plt.rcParams[\"font.size\"] = 18\n",
    "plt.rcParams[\"figure.figsize\"] = [6, 6]\n",
    "# needed for tokenization\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# You need the averaged_perceptron_tagger resource to determine the context of a word in a sentence.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'booking_comments.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1b7f3cbc5112>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"booking_comments.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'None'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mescapechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\\\'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1872\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1874\u001b[0;31m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1875\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'booking_comments.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"booking_comments.csv\", na_values=['None', 'none'], decimal=',', encoding=\"utf8\", escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(f'Le dataset a {df.shape[0]} lignes et {df.shape[1]} colonnes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistiques descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#statistiques descriptives du dataset\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection des variables\n",
    "\n",
    "Notre objectif est de savoir si un commentaire est positif ou négatif.\n",
    "\n",
    "Dans notre analyse, on partira sur 2 hypothèses pour déterminer la polarité des commentaires:  \n",
    "   - calcul de la polarité des commentaires récoltés\n",
    "   - utilisation de la colonne note pour déterminer la polarité (target déjà fourni par Booking)\n",
    "    \n",
    "On conservera donc toutes les variables qui constituent le commentaire final et la note associée.\n",
    "- **TITRE**, \n",
    "- **BONS POINTS**\n",
    "- **MAUVAIS POINTS**\n",
    "- **NOTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data = df.drop(['nom', 'pays', 'favorite', 'date', 'type_etablissement', 'lieu', 'note_etablissement'], axis= 1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# we delete empty rows\n",
    "data = data.dropna(how='all')\n",
    "\n",
    "# we delete the rows where the note is null\n",
    "# since we have 2 hypothesis that we want to test and removing 750 rows over 50k we still have plenty of data\n",
    "data = data.loc[data.note.notna()]\n",
    "\n",
    "# in the titre, bons_points and mauvais points columns we replace nan by empty strings\n",
    "data = data.fillna('')\n",
    "\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On décide aussi de retirer les commentaires générés par Booking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace automatic comments by booking by empty strings\n",
    "commentaires_booking = [\"Ce commentaire n'apparaît pas car il ne respecte pas notre charte.\", \"Ce client n'a pas laissé de commentaire.\"]\n",
    "data.bons_points.loc[data.bons_points.isin(commentaires_booking)] = ''\n",
    "data.mauvais_points.loc[data.mauvais_points.isin(commentaires_booking)] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concaténation des colonnes Titre, Bons Points et Mauvais Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# merge columns\n",
    "data['review'] = data.titre + ' ' + data.bons_points + ' ' + data.mauvais_points\n",
    "data = data[['review', 'note']]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportion de commentaires positifs / négatifs\n",
    "\n",
    "Pour éviter d'avoir un modèle biaisé, on va observer si les commentaires sont équilibrés en fonction de leur note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#diagramme circulaire des modalités de la polarité des commentaires\n",
    "data_percentages = data.note.groupby(data.note > 6).size() / data.shape[0]\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "\n",
    "plt.title(\"Pourcentage des commentaires positifs / négatifs selon la note Booking\", fontsize=18)\n",
    "labels = ['positif', 'négatif']\n",
    "sizes = [data_percentages[1], data_percentages[0]]\n",
    "\n",
    "patches, texts, autotexts = ax1.pie(sizes,  labels=labels, autopct='%1.1f%%', startangle=130, colors = ['#70A288', '#6D2E46'])\n",
    "texts[0].set_fontsize(15)\n",
    "texts[1].set_fontsize(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment préparer son dataset dans un projet NLP ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour optimiser notre modélisation, on souhaite **conserver seulement les mots / caractères les plus pertinents pour déterminer la polarité d'un commentaire**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 étapes :\n",
    "\n",
    "- tokenisation\n",
    "- retrait des stop words\n",
    "- lemmatisation\n",
    "- stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Tokenisation\n",
    "\n",
    "Pour faciliter la compréhension du language naturel par la machine, on doit transformer nos chaînes de caractères en tokens.\n",
    "\n",
    ">**tokens** : mots, ponctuation, symboles\n",
    "\n",
    "La tokenisation basique consiste à séparer le texte par les espaces et la ponctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_review_tokens = (word_tokenize(review) for review in data.review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - retrait des stop words\n",
    "\n",
    "Les conjonctions de coordinations et la ponctuation n'apporte pas d'information pertinente donc pour améliorer la vitesse du modèle on peut les retirer et éviter d'influencer le modèle par leur présence, on les retire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(review):\n",
    "    # remove stop words from the review\n",
    "    stop_words = get_stop_words('french')  \n",
    "    \n",
    "    #remove punctuation\n",
    "    ponctuations = string.punctuation\n",
    "    p = re.compile(r'\\.+')\n",
    "    \n",
    "    # remove stop words, ellipsis, punctuation and words which length is below 2, numbers\n",
    "    review = [p.sub(r'', word) for word in review if word.lower() not in stop_words and word not in ponctuations and not word.isnumeric() and (len(word) > 2 or word == \"ne\")]\n",
    "    \n",
    "    return ' '.join(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_review_tokens = [remove_stopwords(review) for review in get_review_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 -Normalization : stemmatisation & lemmatization\n",
    "\n",
    "La normalisation en NLP consiste à conserver la forme canonique des mots du corpus. On peut utiliser deux méthodes : la lemmatization et le stemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization \n",
    "\n",
    "La lemmatization consiste à analyser le mot selon le contexte d'utilisation et le remplacer par une forme normalisée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 - Etiquetage morpho-syntaxique**\n",
    "\n",
    "On détermine le contexte d'utilisation grâce à l'étiquetage morpho-syntaxique (Part-Of-Speech - POS Tagging ): on essaye d'attribuer une étiquette (tag) à chaque mot correspondant à sa fonctionnalité grammaticale dans la phrase (nom propre, adjectif, etc...)\n",
    "\n",
    "Notre lemmatizer utilise les tags du WordNet, donc il faut lier les tags en sortie de la fonction pos_tag à ceux de WordNet (a, n, r, v).\n",
    "\n",
    "**Part-of-speech tags**\n",
    "\n",
    "|tag de la fonction pos_tag|tag WordNet| signification |\n",
    "|-|-|-|\n",
    "| tags commençant par N | n | nom |\n",
    "| tags commençant par JJ | a | adjectif |\n",
    "| tags commençant par V | v | verbe |\n",
    "| tags commençant par R | r | adverbe |\n",
    "| les autres tags | N/A | N/A |\n",
    "\n",
    "Les tags qui n'entrent dans aucune des catégories, ne seront pas considérés par le lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_POS(review):\n",
    "    # pos tag only work if all the element inside the list aren't empty\n",
    "    review = review.split(' ')\n",
    "    review = list(filter(None, review))\n",
    "    return pos_tag(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_with_pos = [get_POS(review) for review in cleaned_review_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_lemmatizer(review_with_tag):\n",
    "    lemmatizer = FrenchLefffLemmatizer()\n",
    "    \n",
    "    lem_text = []\n",
    "    \n",
    "    for word, pos in review_with_tag:\n",
    "        if pos.startswith('N'):\n",
    "            normalized_word = lemmatizer.lemmatize(word, 'n') \n",
    "        elif pos.startswith('JJ'):\n",
    "            normalized_word = lemmatizer.lemmatize(word, 'a') \n",
    "        elif pos.startswith('V'):\n",
    "            normalized_word =  lemmatizer.lemmatize(word, 'v') \n",
    "        elif pos.startswith('R'):\n",
    "            normalized_word = lemmatizer.lemmatize(word, 'r') \n",
    "        else :\n",
    "            normalized_word = lemmatizer.lemmatize(word) \n",
    "        \n",
    "        lem_text.append(normalized_word)\n",
    "            \n",
    "    return lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_review_tokens = [word_lemmatizer(review) for review in tokens_with_pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le lemmatisation est une opération chronophage, les résultats étant tout de même satisfaisants sans cette étape, nous avons décider de ne pas l'inclure dans notre preprocessing. Nous nous contenterons du stemming, opération beaucoup plus rapide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmatisation\n",
    "\n",
    "Pour éviter d'inclure les variations d'un mot dans notre corpus (ex: vouloir : veux, voulons, veuille, etc..), on va récuperer le radical du mot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_review(review):\n",
    "    stem = FrenchStemmer()\n",
    "    # review = review.split(' ')\n",
    "    return [stem.stem(word) for word in review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_tokens = [stem_review(review) for review in cleaned_review_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nos commentaires sont nettoyés et normalisés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VISUALISATION : Fréquence des mots et nuage de mots\n",
    "\n",
    "Pour observer la fréquence des mots, on va utiliser le corpus **cleaned_review_tokens** qui ne comprend pas la normalization et donc nous permet de voir les mots dans leur forme entière."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to get all words\n",
    "all_words = np.array(cleaned_review_tokens).flatten()\n",
    "all_words = ' '.join(all_words).split() \n",
    "\n",
    "#calculer les 100 mots les plus fréquents\n",
    "nb = 100\n",
    "word_dist = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affichage dans un dataframe des 7 mots les plus fréquents\n",
    "word_frequency = pd.DataFrame(word_dist.most_common(nb), columns=['Word', 'Frequency'])\n",
    "most_frequent = word_frequency.head(10)\n",
    "most_frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogramme des 7 mots les plus fréquents\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.barplot(x=\"Word\",y=\"Frequency\", data=most_frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction pour générer un nuage de mots\n",
    "def wcloud(data,bgcolor,title):\n",
    "    plt.figure(figsize = (100,100))\n",
    "    wc = WordCloud(background_color = bgcolor, max_words = 1000,  max_font_size = 50)\n",
    "    wc.generate(' '.join(data))\n",
    "    plt.imshow(wc)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcloud(all_words,'black','Common Words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de sentiments\n",
    "\n",
    "L'analyse de sentiments consiste à déterminer si un commentaire est positif, neutre ou négatif.\n",
    "\n",
    "### Première approche : calcul de la polarité avec la note Booking\n",
    "\n",
    "Avec les notes Booking, on va faire la répartition suivante et on récupère le dataset commentaires / polarité: \n",
    "\n",
    "0 => 5 : négatif  \n",
    "sup 5 : positif  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = np.array([' '.join(review) for review in normalized_tokens])\n",
    "\n",
    "notes_booking = data.note.copy()\n",
    "notes_booking.loc[notes_booking < 5] = 0\n",
    "notes_booking.loc[notes_booking >= 5] = 1\n",
    "\n",
    "# dataset with preprocessing\n",
    "dataset_note_booking = pd.DataFrame(data= {'review' : reviews, 'polarite' : notes_booking})\n",
    "dataset_note_booking.to_csv('dataset_booking_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deuxieme approche : calcul de la polarité sur les commentaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va utiliser le Pattern Analyzer de TextBlob pour déterminer la polarité d'un commentaire\n",
    "\n",
    "Une polarité de : \n",
    "* \\- 1 à 0 : commentaire négatif\n",
    "* 0 à 1 : commentaire positif\n",
    "\n",
    "Pour avoir une polarité correcte, on va la calculer sur les tokens sans normalisation (utilisation de la liste **cleaned_review_tokens**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment renvoie un tuple avec la polarité et la subjectivite, on veut récuperer juste la première valeur\n",
    "polarite = [TextBlob(review, analyzer=PatternAnalyzer()).sentiment[0] for review in cleaned_review_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets avec la polarité\n",
    "\n",
    "la polarité peut prendre une valeur continue entre -1 et 1. On peut regrouper les valeurs en 2 groupes (0 / 1)\n",
    "et ne pas tenir compte des commentaires neutres (polarité = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_polarite = pd.DataFrame({'review': reviews, 'polarite' : polarite})\n",
    "\n",
    "dataset_polarite = dataset_polarite.loc[dataset_polarite['polarite'] != 0]\n",
    "dataset_polarite.loc[dataset_polarite['polarite'] < 0] = 0\n",
    "dataset_polarite.loc[dataset_polarite['polarite'] > 0] = 1\n",
    "\n",
    "# dataset with preprocessing\n",
    "dataset_polarite.to_csv('dataset_polarite.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrélation entre les notes Booking et la polarité calculée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculons la répartition des commentaires positifs / négatifs selon la polarité obtenue avec TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diagramme circulaire des modalités de la polarité des commentaires\n",
    "data_percentages = dataset_polarite.polarite.value_counts() / dataset_polarite.shape[0]\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "\n",
    "plt.title(\"Pourcentage des commentaires positifs vs négatifs par rapport à la polarié\", fontsize=18)\n",
    "labels = ['positif', 'négatif']\n",
    "sizes = [data_percentages[1], data_percentages[0]]\n",
    "\n",
    "patches, texts, autotexts = ax1.pie(sizes,  labels=labels, autopct='%1.1f%%', startangle=130, colors = ['#70A288', '#6D2E46'])\n",
    "texts[0].set_fontsize(15)\n",
    "texts[1].set_fontsize(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrice de corrélation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'polarite': polarite, 'note': notes_booking.values}\n",
    "polarites = pd.DataFrame(data=d)\n",
    "\n",
    "df_corr = polarites.corr()\n",
    "sns.heatmap(df_corr, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse comparative des modèles de classification\n",
    "\n",
    "On va tester plusieurs modèles pour voir celui qui nous sort les meilleurs résultats : \n",
    "   - Régression logistique\n",
    "   - Naive Bayes : ComplementNB, Multinomial\n",
    "   - Méthodes ensemblistes : XGB, GradientBoosting, RandomForest, AdaBoost\n",
    "   - SVM\n",
    "   \n",
    "### Feature Transformation\n",
    "\n",
    "La majorité des modèles dans Scikit-Learn ne prennent pas en entrée des chaînes de caractère, il faut transformer nos variables en données numériques au préalable.\n",
    "\n",
    "Il y a plusieurs méthodes que l'on peut utiliser: \n",
    "\n",
    "#### Count Vectorizer\n",
    "\n",
    "> DEFINITION\n",
    "\n",
    "#### Tf-idf Transformer\n",
    "\n",
    "> DEFINITION\n",
    "\n",
    "#### Truncated SVD\n",
    "\n",
    "> DEFINITION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la partie Analyse des données, nous avons détaillé les étapes du preprocessing dans un projet NLP mais on peut aussi les synthétiser dans une Pipeline.  \n",
    "\n",
    "On partira donc des datasets sans preprocessing : **dataset_polarite_before_nlp_prepro** & **dataset_note_booking_before_nlp_prepro**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(db_name, model_name, train_name, model, X_train, y_train, default_params = None, grid_search_params = None):\n",
    "    '''\n",
    "    run model with different feature transformations + with or without GridSearch\n",
    "    '''\n",
    "    # measures the time taken by the model to run\n",
    "    start_time = time.time()\n",
    "\n",
    "    # get model\n",
    "    if default_params:\n",
    "        clf = model(**default_params)\n",
    "    else : \n",
    "        clf = model()\n",
    "    \n",
    "    # fit model with or without GridSearchCV\n",
    "    if grid_search_params :\n",
    "        grid_clf = GridSearchCV(clf, grid_search_params, n_jobs=-1)\n",
    "        clf = grid_clf.fit(X_train, y_train)\n",
    "        best_params = clf.best_params_\n",
    "        print(best_params)\n",
    "    else:\n",
    "        clf.fit(X_train, y_train)\n",
    "        best_params = None\n",
    "            \n",
    "    # get predictions\n",
    "    y_pred = clf.predict(X_train)\n",
    "    \n",
    "    # we will record some metrics in a CSV file for presentation\n",
    "    duration = time.time() - start_time\n",
    "    accuracy = np.mean(cross_val_score(clf, X_train, y_train, cv=5)) # avg score on the CV\n",
    "    precision = precision_score(y_train, y_pred, average=None)\n",
    "    recall = recall_score(y_train, y_pred, average=None)\n",
    "    f1score = f1_score(y_train, y_pred, average=None)\n",
    "    results = [[db_name, model_name, train_name, accuracy, precision, recall, f1score, best_params, duration ]]\n",
    "    \n",
    "    # save to csv\n",
    "    cols = ['dataset', 'model_name', 'feature transformation', 'accuracy', 'precision', 'recall', 'f1_score', 'best paramaters', 'duration']\n",
    "    \n",
    "    try:\n",
    "        backup = pd.read_csv('booking_models_metrics.csv')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        backup = pd.DataFrame([], columns=cols)\n",
    "    \n",
    "    model_metrics = pd.DataFrame(results, columns=cols)\n",
    "    \n",
    "    backup = pd.concat([backup, model_metrics])\n",
    "    backup.to_csv('booking_models_metrics.csv', index=False)\n",
    "    \n",
    "    return backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_results(name_dataset, dataset):\n",
    "    '''\n",
    "    for a given dataset, run all predefined models and paramaters and save the results into a CSV file\n",
    "    '''\n",
    "    \n",
    "    # get split data\n",
    "    X = dataset['review']\n",
    "    y = dataset['polarite']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    \n",
    "    # get different types of X_train according to feature transformation\n",
    "    train_data = {}\n",
    "    \n",
    "    # CountVectorizer + Tf-idf\n",
    "    pipe_tfidf = make_pipeline(CountVectorizer(), TfidfTransformer())\n",
    "    X_train_tfidf = pipe_tfidf.fit_transform(X_train).toarray()\n",
    "    train_data['CV + TF-IDF'] = X_train_tfidf\n",
    "    \n",
    "    \n",
    "    # CountVectorizer + N-gram\n",
    "    pipe_ngram = make_pipeline(CountVectorizer(min_df=0.0005, ngram_range=(1, 2)), TfidfTransformer())\n",
    "    X_train_ngram = pipe_ngram.fit_transform(X_train).toarray()\n",
    "    train_data['CV(n-gram) + TF-IDF'] = X_train_ngram\n",
    "    \n",
    "    \n",
    "    # CountVectorizer + Tf-idf Truncated SVD\n",
    "    pipe_svd_tfidf = make_pipeline(CountVectorizer(), TfidfTransformer(), TruncatedSVD(n_components=300))\n",
    "    X_train_svd = pipe_svd_tfidf.fit_transform(X_train)\n",
    "    train_data['CV + TF-IDF + SVD'] = X_train_svd\n",
    "    \n",
    "    # list models\n",
    "    models = {\n",
    "        'Regression logistique' : LogisticRegression\n",
    "        'NB : Naive Bayes' : MultinomialNB, \n",
    "        'Gradient Boosting ' : GradientBoostingClassifier,\n",
    "        'Random Forest' : RandomForestClassifier,\n",
    "        'XGB': XGBClassifier,\n",
    "        'SVC' : SVC,\n",
    "        'AdaBoost': AdaBoostClassifier\n",
    "    }\n",
    "    \n",
    "    # related params for GridSearch and random_state\n",
    "    params = {\n",
    "        'Regression logistique' : [{'penalty' : ['l1', 'l2']}, {'random_state' : 0}]\n",
    "        'NB : Naive Bayes' : [ None, None ],\n",
    "        'Gradient Boosting' : [{'learning_rate' : [0.05, 0.01, 0.2], \n",
    "                                'criterion': [\"friedman_mse\", \"mae\", \"mse\"],\n",
    "                                'n_estimators' : [50, 200],\n",
    "                                'max_depth': [6, 30, 50]}, {'random_state' : 0}],\n",
    "        'Random Forest' : [{'boostrap' : [True, False], \n",
    "                            'criterion' : ['gini', 'entropy'], \n",
    "                            'n_estimators' : [50, 200],\n",
    "                            'max_depth': [6, 30, 50]}, {'random_state' : 0}], \n",
    "        'XGB': [{'learning_rate' : [0.05, 0.01, 0.2],\n",
    "                 'max_depth' : [6, 30, 50],\n",
    "                 'n_estimators' : [50, 200]}, {'random_state' : 0}],\n",
    "        'SVC' : [{ 'C': [1, 5, 10, 50],\n",
    "                  'gamma': [0.0001, 0.0005, 0.001, 0.005],\n",
    "                 'kernel': ['rbf','sigmoid']}, \n",
    "                 {'random_state' : 0}],\n",
    "        'AdaBoostClassifier' : [ {'base_estimator' : [MultinomialNB, SVC],\n",
    "                                  'n_estimators' : [50, 200]}, \n",
    "                                {'random_state' : 0}]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # run models with different parameters and different feature extraction methods\n",
    "    for name_model, model in models.items():\n",
    "        for name_X_train, X_train in train_data.items():\n",
    "            print(name_dataset, name_X_train, name_model, params[name_model][0], params[name_model][1])\n",
    "            try : \n",
    "                run_model(name_dataset, \n",
    "                          name_model, \n",
    "                          name_X_train, \n",
    "                          model, \n",
    "                          X_train, \n",
    "                          y_train, \n",
    "                          default_params = params[name_model][1], \n",
    "                          grid_search_params = params[name_model][0])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with one model + feature transformation Count Vectorizer + tf-idf\n",
    "X = dataset_note_booking['review']\n",
    "y = dataset_note_booking['polarite']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    \n",
    "pipe_tfidf = make_pipeline(CountVectorizer(), TfidfTransformer())\n",
    "X_train_tfidf = pipe_tfidf.fit_transform(X_train).toarray()\n",
    "\n",
    "run_model('note_booking', \n",
    "          'multinomial', \n",
    "          'cv + tf-idf', \n",
    "          LogisticRegression, \n",
    "          X_train_tfidf, \n",
    "          y_train, \n",
    "          default_params = {'random_state' : 0}, \n",
    "          grid_search_params = {'penalty' : ['l1', 'l2']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on récupère tous les modèles et leurs métriques dans le fichier CSV booking_models_metrics.csv\n",
    "get_models_results('note_booking', dataset_note_booking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_models_results('note_polarite', dataset_polarite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Résultats et Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mettre le CSV obtenu après l'entraînement des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Conclusions et difficultés rencontrées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "TBD\n",
    "\n",
    "#### Scraping\n",
    "\n",
    "- **gérer les coupures internet et les code recaptcha** : il faut relancer le process automatiquement après coupure ou lorsqu'un popup 'Etes-vous humain' empêche toute interaction.\n",
    "- **booking.com ouvre forcément les pages établissement sur un nouvel onglet**: cela ralentit le code puisqu'il faut prendre en compte le temps de latence et les codes intempésitfs de booking (cookie / recaptcha)\n",
    "- **gérer le multiprocessing** : pour récupérer le plus de données possibles, on a utilisé le multiprocessing. Mais on s'est rendu compte que la gestion des exceptions est ici primordiale parce que malgré les exceptions, le code doit continuer de scraper le site. Ce qui compte c'est d'en scraper le plus possible et non de pouvoir scrapper tous les commentaires existants. \n",
    "- **récupération des commentaires négatifs**\n",
    "\n",
    "### Analyse des données\n",
    "\n",
    "TBD - Williams ?\n",
    "\n",
    "### Machine Learning\n",
    "\n",
    "TBD - Valérie ?\n",
    "\n",
    "### Création de l'application\n",
    "\n",
    "L'application Flask nous permet de soumettre un commentaire et de voir la prédiction du modèle trouvé lors de l'étape de Machine Learning. Les commentaires sont sauvegardés dans une base de données."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python37764bitbaseconda04fa0fea65bf4a10b4dfdb5bab849fc4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

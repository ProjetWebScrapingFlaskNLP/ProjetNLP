{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, ElementClickInterceptedException\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "import pprint\n",
    "import re\n",
    "import datetime\n",
    "import locale\n",
    "from selenium.webdriver.chrome.options import Options \n",
    "import chromedriver_binary\n",
    "from stopit import SignalTimeout as Timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fr_FR.UTF-8'"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# necessary to get french date\n",
    "locale.setlocale(locale.LC_ALL, 'fr_FR.UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_persistent(backup_link, cols, new_data):\n",
    "    # persistent data\n",
    "    try:\n",
    "        backup = pd.read_csv(backup_link)\n",
    "    except FileNotFoundError:\n",
    "        backup = pd.DataFrame([], columns=cols)\n",
    "        \n",
    "    # if new_data isn't a dataframe, we create a DataFrame from it\n",
    "    if not isinstance(new_data, pd.core.frame.DataFrame):\n",
    "        try:\n",
    "            new_data = pd.DataFrame(new_data, columns=cols)\n",
    "        except:\n",
    "            raise\n",
    "    \n",
    "    new_backup = pd.concat([backup, new_data])\n",
    "    new_backup.to_csv(backup_link, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accessibility(browser):\n",
    "    \"\"\"\n",
    "    if cookie banner and robot detection are present, remove them\n",
    "    \"\"\"\n",
    "    \n",
    "    # if the are you human popup appears, remove it\n",
    "    try :\n",
    "        robot_detection = browser.find_element_by_id('botdetect_abu_nip__overlay')\n",
    "        return False\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    \n",
    "    # remove cookie banner because it takes almost 50% height of the page\n",
    "    try:\n",
    "        cookie_banner = browser.find_element_by_css_selector('#cookie_warning button')\n",
    "        cookie_banner.click()\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_page(url):\n",
    "    while True:\n",
    "        try:\n",
    "            browser = webdriver.Chrome()\n",
    "            browser.get(url)\n",
    "            time.sleep(3) # let's the DOM load\n",
    "            \n",
    "            logo = browser.find_element_by_css_selector('#logo_no_globe_new_logo') # if it can't get the logo, it means the page isn't loaded\n",
    "            robot_detection = accessibility(browser)\n",
    "    \n",
    "            if robot_detection :\n",
    "                break\n",
    "        except:\n",
    "            browser.close() # reload the page\n",
    "    \n",
    "    return browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to get the hotel links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hotels_links_by_page(browser, query):\n",
    "    \"\"\"\n",
    "    get all hotel links inside a webpage\n",
    "    \"\"\"\n",
    "    \n",
    "    # we can compare it to a previous backup if we have it\n",
    "    \n",
    "    try:\n",
    "        backup_links = pd.read_csv(f'backup_hotel_links_{query}.csv')\n",
    "    except:\n",
    "        backup_links = None\n",
    "        print('no backup file')\n",
    "    \n",
    "    hotel_links_array = []\n",
    "\n",
    "    # get link for each hotel in the page\n",
    "    for hotel in browser.find_elements_by_css_selector('#hotellist_inner .sr_item'):\n",
    "        \n",
    "        try:\n",
    "            # we only want hotel with reviews\n",
    "            has_rating = hotel.find_elements_by_css_selector('.bui-review-score__badge')\n",
    "            \n",
    "            if len(has_rating) > 0 :\n",
    "                link = hotel.find_element_by_css_selector('h3 .hotel_name_link').get_attribute('href')\n",
    "                \n",
    "                existing_link = []\n",
    "                \n",
    "                if backup_links is not None:\n",
    "                    existing_link = backup_links.loc[backup_links['link'] == link]\n",
    "                \n",
    "                if len(existing_link) == 0: # if not in database or if the backup file doesn't exist\n",
    "                    # we want the short version of the link to save space in the csv\n",
    "                    pattern = re.compile(r'(.+)\\?')\n",
    "                    result = pattern.match(link)\n",
    "                    short_link = result.group(1)\n",
    "                    hotel_links_array.append([short_link, 0])\n",
    "                    \n",
    "        except StaleElementReferenceException as e:\n",
    "            print(e)\n",
    "       \n",
    "    # persistent data\n",
    "    make_data_persistent(f'backup_hotel_links_{query}.csv', ['link', 'has_been_scrapped'], hotel_links_array)\n",
    "    \n",
    "    return hotel_links_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_hotel_links(browser, query):\n",
    "    \"\"\"\n",
    "    loop through all results pages to get hotel links to scrap\n",
    "    \"\"\"\n",
    "    \n",
    "    all_links = []\n",
    "    \n",
    "    while True:\n",
    "        time.sleep(2) # to prevent stale elements\n",
    "        try :\n",
    "            all_links.append(get_hotels_links_by_page(browser, query))\n",
    "            next_btn = browser.find_element_by_css_selector('.bui-pagination__next-arrow:not(.bui-pagination__item--disabled) .bui-pagination__link')\n",
    "            next_btn.click()\n",
    "        except NoSuchElementException:\n",
    "            print('no more results') # no more results\n",
    "            break\n",
    "        except StaleElementReferenceException as e:\n",
    "            print(e)\n",
    "            \n",
    "    # flatten the list of all links\n",
    "    all_links = [link[0] for links_by_page in all_links for link in links_by_page]\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to fetch comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_for_comment_item(review, col):\n",
    "    \"\"\"\n",
    "    we optimize the process of collecting the values of reviews elements (title, name, rating, etc..)\n",
    "    \"\"\"\n",
    "    \n",
    "    items = {\n",
    "        'column': ['nom', 'pays', 'favorite', 'date', 'titre', 'bons_points', 'mauvais_points', 'note'],\n",
    "        'css_selector' : ['.bui-avatar-block__title', '.bui-avatar-block__subtitle', '.c-review-block__badge', '.c-review-block__date', '.c-review-block__title', '.c-review__row:not(.lalala) .c-review__body', '.lalala .c-review__body', '.bui-review-score__badge'],\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # we get the index of the column we are dealing with, to get the related css\n",
    "        col_idx = items['column'].index(col)\n",
    "        css = items['css_selector'][col_idx]\n",
    "        \n",
    "        # we get the value related to the column\n",
    "        item = review.find_element_by_css_selector(css).text\n",
    "            \n",
    "        # date info is shown like this : 'Commentaire envoyÃ© le DD/MM/YYYY', \n",
    "        # we only need the date inside the text\n",
    "        if col == 'date':\n",
    "            try: \n",
    "                pattern = re.compile(r'\\d{1,2}\\s\\w+\\s\\w{4}')\n",
    "                result = pattern.search(item)\n",
    "                date_str = result.group()\n",
    "                item = datetime.datetime.strptime(date_str, \"%d %B %Y\")\n",
    "            except AttributeError:\n",
    "                item = 'None'\n",
    "                \n",
    "        # in case we are dealing with the Choix de l'utilisateur / favorite column\n",
    "        # we don't need the text, if favorite element present in the block => 1 otherwise 0\n",
    "        if col == 'favorite' and item:\n",
    "            item = 1\n",
    "                \n",
    "        return item\n",
    "    \n",
    "    except NoSuchElementException: # in case the item can't be fetched\n",
    "         if col == 'favorite':\n",
    "            return 0\n",
    "         else :\n",
    "            return 'None'\n",
    "    except: # unexpected error\n",
    "        print('an error occured')\n",
    "        return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment_data(review, etablissement, cols):\n",
    "    \"\"\"\n",
    "    get the comment data\n",
    "    \"\"\"\n",
    "    \n",
    "    new_row = []\n",
    "    \n",
    "    # we can get the first 8 cells from the review block itself\n",
    "    for col in cols[:8]:\n",
    "        new_row.append(get_value_for_comment_item(review, col))\n",
    "    \n",
    "    # we collect also the data about the accomodation\n",
    "    new_row.append(etablissement['type'])\n",
    "    new_row.append(etablissement['lieu'])\n",
    "    new_row.append(etablissement['note'])\n",
    "    \n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(browser, etablissement, query, link):\n",
    "    \"\"\"\n",
    "    comments can be displayed on several pages. we want max 300 reviews per hotel\n",
    "    \"\"\"\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    # create new dataframe to save the comments into a backup csv file\n",
    "    cols = ['nom', 'pays', 'favorite', 'date', 'titre', 'bons_points', 'mauvais_points', 'note', 'type_etablissement', 'lieu', 'note_etablissement']\n",
    "    data = pd.DataFrame([], columns=cols)\n",
    "    \n",
    "    while True:\n",
    "        time.sleep(2) # wait till booking get the following comments\n",
    "        \n",
    "        # we get each review and call the script to get its content\n",
    "        for review in browser.find_elements_by_css_selector('.review_list .review_list_new_item_block'): \n",
    "        \n",
    "            # scroll to the review\n",
    "            browser.execute_script('arguments[0].scrollIntoView({behavior: \"smooth\", block: \"end\", inline: \"nearest\"});', review)\n",
    "        \n",
    "            # we add the content of the review\n",
    "            try:\n",
    "                data.loc[len(data.index)] = get_comment_data(review, etablissement, cols)\n",
    "            except:\n",
    "                data.loc[len(data.index)] = [None] * 11\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "        # when we are done with a section of comments, we check if we have at least 300 comments\n",
    "        if count >= 300:\n",
    "            break \n",
    "            \n",
    "        # otherwise, we open a new comments panel\n",
    "        try :\n",
    "            next_btn = browser.find_element_by_css_selector('#review_list_score_container .bui-pagination__next-arrow:not(.bui-pagination__item--disabled) a')\n",
    "            next_btn.click()\n",
    "        except NoSuchElementException:\n",
    "            print('no more comments to load')\n",
    "            break\n",
    "        except StaleElementReferenceException as e:\n",
    "            print(e)\n",
    "    \n",
    "    # we save the scrapped comments in a backup csv file\n",
    "    make_data_persistent(f'backup_booking_{query}.csv', cols, data)\n",
    "    \n",
    "    # update the hotel_links list of the query, so when we have to do scrap again we can resume at the right spot\n",
    "    try:\n",
    "        backup_links = pd.read_csv(f'backup_hotel_links_{query}.csv')\n",
    "        mask = backup_links['link'] == link\n",
    "        backup_links.loc[mask, 'has_been_scrapped'] = 1\n",
    "        backup_links.to_csv(f'backup_hotel_links_{query}.csv', index=False)\n",
    "    except :\n",
    "        print('unexpected error')\n",
    "    browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_comments_panel(url):\n",
    "    \"\"\"\n",
    "    get hotel page, open French comments section and return the location\n",
    "    \"\"\"\n",
    "    \n",
    "    # get page - in case the network isn't available or an issue with robot detection, \n",
    "    # we reload the page until we can access it\n",
    "    browser = reload_page(url)\n",
    "    \n",
    "    # open reviews panel\n",
    "    try:\n",
    "        btn_cmt = browser.find_element_by_id('show_reviews_tab')\n",
    "        btn_cmt.click()\n",
    "    except NoSuchElementException as e:\n",
    "        print(e) # it means there is no review\n",
    "        return False\n",
    "    except ElementClickInterceptedException as e:\n",
    "        print(e, 'already open') # if the webpage has already been visited by us\n",
    "        \n",
    "    #get only french reviews\n",
    "    time.sleep(1) # wait a little bit till the checkbox is available\n",
    "    try:\n",
    "        btn_french = browser.find_element_by_css_selector('.language_filter_checkbox[value=\"fr\"] + span')\n",
    "        btn_french.click()\n",
    "    except NoSuchElementException :\n",
    "        # there is no French review - we continue with another accomodation\n",
    "        browser.close()\n",
    "        return False\n",
    "        \n",
    "    # it has to take into account the language change\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # get info about accomodation\n",
    "    try:\n",
    "        etablissement = {\n",
    "            'nom': browser.find_element_by_css_selector('.hp__hotel-name').text,\n",
    "            'type' : browser.find_element_by_css_selector('.hp__hotel-name span').text,\n",
    "            'note': browser.find_element_by_css_selector('.reviewFloater .bui-review-score__badge').get_attribute('innerHTML'), # sometimes it is hidden\n",
    "            'lieu' : browser.find_element_by_css_selector('.sb-destination__input').get_attribute(\"value\")\n",
    "        }\n",
    "    except NoSuchElementException as e:\n",
    "        print(e, 'no location listed')\n",
    "        etablissement = {\n",
    "            'nom': 'None',\n",
    "            'type' : 'None',\n",
    "            'note': 'None', # sometimes it is hidden\n",
    "            'lieu' : 'None'\n",
    "        }\n",
    "        \n",
    "    return etablissement, browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_booking(query):\n",
    "    # browser = webdriver.Chrome()\n",
    "    \n",
    "    # get page - in case the network isn't available or an issue with robot detection, \n",
    "    # we reload the page until we can access it\n",
    "    browser = reload_page(\"https://booking.com\")\n",
    "    \n",
    "    # send query value\n",
    "    search_input = browser.find_element_by_id('ss')\n",
    "    search_input.send_keys(query)\n",
    "    \n",
    "    # btn submit\n",
    "    btn_submit = browser.find_element_by_class_name('sb-searchbox__button')\n",
    "    btn_submit.click()\n",
    "    \n",
    "    # get hotel links that have to be scrapped - if we stopped in the process before, \n",
    "    # it will resume where it stopped\n",
    "    \n",
    "    try :\n",
    "        hotel_links = pd.read_csv(f'backup_hotel_links_{query}.csv')\n",
    "        mask = hotel_links['has_been_scrapped'] == 0\n",
    "        all_links = hotel_links.loc[mask, 'link']\n",
    "    except : \n",
    "        print('the backup file doesnt exist')\n",
    "        all_links = get_all_hotel_links(browser, query)\n",
    "        \n",
    "    # close Chrome\n",
    "    browser.close()\n",
    "    \n",
    "    #get comments\n",
    "    for link in all_links:\n",
    "        try: # 8 min to get all comments, otherwise we go to the next link\n",
    "            with Timeout(480.0) as timeout_ctx:\n",
    "                etablissement, new_browser = open_comments_panel(link)\n",
    "                get_comments(new_browser, etablissement, query, link)\n",
    "        except:\n",
    "            browser.close()\n",
    "            print('error')\n",
    "    \n",
    "    # when issue with internet, we have this error : NoSuchElementException  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect_to_booking(\"Cap Breton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no more results\n",
      "no more results\n",
      "no more results\n",
      "no more results\n"
     ]
    }
   ],
   "source": [
    "with multiprocessing.Pool() as pool:\n",
    "    pool.map(connect_to_booking, ['Cannes', 'Biarritz', 'Brest', 'Montpellier'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
